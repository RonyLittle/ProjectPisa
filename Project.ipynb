{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24bdeac4-d42a-46c9-9740-42a9ae74a059",
   "metadata": {},
   "source": [
    "# Deep Analysis of PISA 2022 Data: Interrelations of Academic Achievement with Sociocultural Factors\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This project undertakes an in-depth analysis of the Program for International Student Assessment (PISA), an initiative orchestrated by the Organisation for Economic Co-operation and Development (OECD). PISA is a worldwide study designed to evaluate the educational performance of 15-year-old students and their preparedness for real-world challenges beyond the formal school curriculum. This assessment, which takes place every three years, measures the abilities of students in critical cognitive domains including reading literacy, mathematical literacy, and scientific literacy.\n",
    "\n",
    "The significance of PISA lies in its role as a global benchmark for evaluating education systems worldwide by comparing the skills and knowledge of students across different countries. This comparison helps in identifying effective educational practices and policies. By focusing on how well young adults can apply their knowledge to real-life situations, PISA provides valuable insights into the effectiveness of schooling in different regions and aids in policymaking to enhance educational outcomes.\n",
    "\n",
    "Through rigorous and standardized testing methodologies, PISA evaluates not just rote memorization, but the ability of students to think critically, solve complex problems, and make reasoned decisions. It thus provides a comprehensive picture of students' capabilities in handling the demands of future academic and occupational settings. The insights drawn from PISA data are instrumental for educators, policymakers, and stakeholders in crafting strategies that improve educational standards and foster an environment that nurtures the full potential of every learner.\n",
    "\n",
    "## Data Source\n",
    "\n",
    "For this project, we harness the most recent 2022 data set obtained from the \"Student Questionnaire Data File\" available on the official OECD website. This file is a comprehensive repository of data, meticulously compiled from the responses of students and their parents across various countries. It serves as a foundational element for our analysis, providing both performance scores and a diverse range of background variables. These variables facilitate a deep dive into several critical aspects of the educational landscape:\n",
    "\n",
    "- **Demographic and Socio-economic Profiles**: This category includes detailed demographic information about students and their familial backgrounds, capturing data points such as age, gender, immigration status, as well as the educational levels and socio-economic statuses of their parents. These variables are crucial for understanding the diverse contexts from which students hail and how these factors might influence their educational achievements.\n",
    "\n",
    "- **Educational Performance**: The data file provides scores across fundamental academic subjects—mathematics, reading, and science. These scores are offered as plausible values, which are multiple imputed scores reflecting students’ abilities derived from their test performances. This approach allows for more nuanced statistical analyses and helps in understanding the variability in student performance across different educational systems.\n",
    "\n",
    "- **Learning Environment and Behaviors**: This section sheds light on the non-academic aspects of students' school life, encompassing their study habits, self-reported motivational attitudes, their sense of belonging at their school, and their experiences with bullying. Such data points are invaluable for assessing the psycho-social dimensions that influence educational outcomes.\n",
    "\n",
    "- **Digital Literacy and Resources**: In today’s digital age, access to technology is a significant factor in educational success. This variable assesses the availability and usage of information and communication technologies (ICT) at students’ homes. It examines how these tools are integrated into the learning process and their impact on students' educational performance.\n",
    "\n",
    "By analyzing these detailed and multi-faceted data, this project aims to uncover patterns and trends that offer insights into the factors that most significantly impact student learning outcomes. These insights can help policymakers, educators, and communities to design targeted interventions that enhance educational equity and effectiveness.\n",
    "\n",
    "## Core Analytical and Advanced Programming Methods\n",
    "\n",
    "The project harnesses a diverse array of data analysis techniques tailored specifically to dissect the complex interactions within educational data provided by the PISA 2022 dataset. Here’s a rundown of the primary methods applied:\n",
    "\n",
    "- **Statistical Analysis**:\n",
    "  - **Descriptive Statistics**: Summarize data features like central tendency, variability, and distribution shapes.\n",
    "  - **Shapiro-Wilk Test**: Assess the normality of data distributions, crucial for the validity of many other statistical tests.\n",
    "  - **Correlation Analysis**: Determine the strength and direction of relationships between variables using Pearson’s correlation coefficient.\n",
    "  - **Regression Analysis**: Linear regression to predict educational outcomes and polynomial regression to capture non-linear relationships.\n",
    "\n",
    "- **Machine Learning**:\n",
    "  - **Random Forest Regression**: Utilized to predict outcomes based on multiple input variables and to assess feature importance, providing insights into which factors most significantly impact student performance.\n",
    "  - **Cross-Validation**: Enhance model validation through k-fold cross-validation, ensuring the model’s robustness and generalizability.\n",
    "\n",
    "- **Deep Learning**:\n",
    "  - **Neural Networks**: Deploy neural networks to model complex, non-linear interactions between variables. The multi-layer perceptron architecture facilitates the exploration of deeper patterns in the data, instrumental in uncovering hidden insights.\n",
    "\n",
    "- **Data Visualization**:\n",
    "  - **Plotly and Seaborn**: Generate interactive graphs and static plots to visualize data distributions, correlations, and regression outcomes effectively. This includes creating heatmaps for correlation matrices, scatter plots for regression analysis, and treemaps for hierarchical data exploration.\n",
    "\n",
    "- **Advanced Data Processing**:\n",
    "  - **Handling Missing Data**: Techniques such as imputation and dropping rows/columns to clean the dataset, ensuring the integrity of the analyses.\n",
    "  - **Feature Engineering**: Includes generating polynomial features for regression models and encoding categorical variables to prepare the dataset for machine learning.\n",
    "\n",
    "These methods collectively facilitate a thorough exploration of the intricate dynamics influencing educational achievements across various demographics and socio-economic backgrounds. Through the intelligent application of these techniques, the project aims to provide actionable insights that could inform policy-making and educational strategies.\n",
    "\n",
    "## Project Aim\n",
    "\n",
    "The overarching goal of this project is to dissect and understand the multitude of factors that influence educational outcomes for students assessed in the PISA 2022 survey. By leveraging a comprehensive dataset provided by the OECD, this analysis seeks to uncover the nuanced interplay between students' academic performances and their demographic, socio-economic, and environmental contexts.\n",
    "\n",
    "### Objectives:\n",
    "1. **Identify Key Factors**: Determine the primary demographic, socio-economic, and educational variables that significantly impact students' scores in mathematics, reading, and science.\n",
    "2. **Model Educational Outcomes**: Utilize advanced statistical methods and machine learning algorithms to predict educational outcomes and interpret the relative importance of each predictor. This will include assessing the impact of factors such as access to technology, parental education levels, and school environments on student performance.\n",
    "3. **Evaluate Policy Implications**: Analyze the data to provide evidence-based recommendations to educational authorities and policymakers. The aim is to identify potential areas for intervention that could lead to improvements in educational equity and effectiveness.\n",
    "4. **Promote Educational Equity**: Explore how differences in educational access and quality affect performance across various groups, aiming to highlight disparities and recommend strategies for promoting inclusivity and fairness in education.\n",
    "\n",
    "### Impact:\n",
    "The insights derived from this project are intended to inform and enhance educational policies and practices worldwide. By understanding the factors that drive educational success, stakeholders can implement targeted interventions to support underperforming groups, optimize educational resources, and ultimately raise the standard of education provided to all students. Furthermore, this project aims to stimulate ongoing dialogue among educators, policymakers, and the academic community about how best to harness data-driven insights for educational planning and reform.\n",
    "\n",
    "In summary, this project not only aims to analyze the data from the PISA 2022 survey comprehensively but also seeks to translate these analyses into practical strategies that can lead to real and sustainable improvements in educational systems globally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ae37d5f-e3c9-4f30-a8ae-15e1cd19a8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6cdbc8-3bcb-479f-b36a-9f575888e50e",
   "metadata": {},
   "source": [
    "## Foundational Libraries\n",
    "\n",
    "- **warnings**: Utilized to manage warnings during runtime, particularly for suppressing specific warning categories that might clutter the output, ensuring a clean presentation of results. This is especially useful in a data science context to ignore routine warnings generated by third-party libraries without affecting the interpretation of code execution.\n",
    "\n",
    "- **numpy (np)**: A cornerstone for numerical computing in Python, numpy offers comprehensive support for arrays and matrices alongside a vast library of mathematical functions to operate on these data structures. In our project, it is indispensable for handling numerical operations on arrays efficiently, which underpins various data manipulation tasks.\n",
    "\n",
    "- **pandas (pd)**: Essential for data manipulation and analysis, pandas provides data structures and operations for manipulating numerical tables and time series. This library is crucial in our project for reading, writing, and processing data from various file formats. It enables sophisticated data manipulation capabilities such as merging, reshaping, selecting, as well as robust handling of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d92c2e22-be95-4415-8a6f-86450c3d0779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy.stats import shapiro, randint as sp_randint\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.graphics.regressionplots import add_lowess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ce3468-e5b0-41f9-b93b-cb4af502bed7",
   "metadata": {},
   "source": [
    "## Statistical and Machine Learning Libraries\n",
    "\n",
    "- **scipy.stats, shapiro, sp_randint**: This suite of tools from the SciPy library supports the generation of random variables, conducting statistical tests, and exploratory data analysis. `shapiro` tests for normality, essential for validating assumptions in many statistical models, while `sp_randint` is used for generating discrete random numbers for hyperparameter tuning.\n",
    "\n",
    "- **sklearn.model_selection**: Includes submodules like `train_test_split` for dividing data into training and test sets, `RandomizedSearchCV` for optimizing model parameters through random search, and `cross_val_score` for evaluating a model's performance using cross-validation.\n",
    "\n",
    "- **sklearn.preprocessing**: Contains `PolynomialFeatures` for generating polynomial and interaction features and `StandardScaler` for feature scaling by standardizing variables. These preprocessing tools are vital for enhancing model performance and accuracy.\n",
    "\n",
    "- **sklearn.linear_model**: Provides models like `LinearRegression` for standard linear regression analysis and `SGDRegressor` for linear models fitted by stochastic gradient descent, a practical approach for large datasets.\n",
    "\n",
    "- **sklearn.ensemble**: Features `RandomForestRegressor`, an ensemble method based on randomized decision trees, known for its high accuracy and robustness against overfitting, particularly useful for regression tasks in complex datasets.\n",
    "\n",
    "- **sklearn.metrics**: Includes performance metrics such as `mean_squared_error` for quantifying the accuracy of regression models and `r2_score` for assessing the proportion of variance captured by the model.\n",
    "\n",
    "- **statsmodels.api**: Offers classes and functions for the estimation of different statistical models, as well as for conducting statistical tests and data exploration. An essential tool for in-depth statistical analysis, often used for regression diagnostics, time-series analysis, and hypothesis testing.\n",
    "\n",
    "- **statsmodels' diagnostic tools**: `het_breuschpagan` tests for heteroscedasticity, `durbin_watson` assesses autocorrelation in residuals from a regression, `variance_inflation_factor` evaluates multicollinearity, and `add_lowess` (locally weighted scatterplot smoothing) is useful for trend fitting in regression diagnostics. These tools provide deeper insights into the model's assumptions and performance, ensuring robust statistical inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "015bb99c-a8b6-4404-ae8c-bccba480c589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2c6cd5-26e0-4742-ac89-4460ada1630e",
   "metadata": {},
   "source": [
    "## Deep Learning Frameworks: TensorFlow and Keras\n",
    "\n",
    "- **TensorFlow**: An open-source library developed by Google for numerical computation and machine learning. TensorFlow provides a broad toolkit for developing and training machine learning models, including powerful features for deep learning. It is used in this project to harness complex patterns in the data that simpler models might miss, especially beneficial for large datasets with intricate features.\n",
    "\n",
    "- **tensorflow.keras.models**: Contains `Sequential`, which is a linear stack of layers used for creating models. The Sequential model is straightforward to understand and use, which is particularly useful for standard deep learning applications where layers are added in sequence.\n",
    "\n",
    "- **tensorflow.keras.layers**: Provides various layers, including `Dense`, which is a regular densely-connected neural network layer. Dense layers are fundamental in neural networks for learning high-level patterns in large data sets and are used extensively in our models to process and learn from educational data. Each neuron in a Dense layer receives input from all neurons of the previous layer, thus being well-suited for pattern recognition tasks found in complex datasets like PISA.\n",
    "\n",
    "These tools are integral to building neural network architectures, facilitating the exploration and implementation of deep learning models that can potentially reveal nonlinear relationships and interactions not detectable by traditional statistical methods. This capability is particularly valuable in educational research, where interactions between variables are complex and multidimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a53dc3f-ccf4-4c29-ac1a-1f5ddb44b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import squarify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a576052-a06e-4620-946d-2ee6ed74f824",
   "metadata": {},
   "source": [
    "## Data Visualization Libraries\n",
    "\n",
    "- **Matplotlib (plt)**: A powerful plotting library for Python, Matplotlib is fundamental for creating static, interactive, and animated visualizations in Python. In this project, Matplotlib is utilized primarily for generating histograms, scatter plots, and more complex visualizations like residual plots, providing a traditional and detailed approach to data visualization.\n",
    "\n",
    "- **Seaborn (sns)**: Built on top of Matplotlib, Seaborn extends its functionality, making it easier to generate complex visualizations with more attractive and informative statistical graphics. This library is used for creating enhanced visualizations such as heatmaps for correlation matrices, which are crucial for identifying relationships between variables in the dataset.\n",
    "\n",
    "- **Plotly (go, px, ff)**: A modern platform for creating interactive plots and dashboards. Plotly's Python graphing libraries `plotly.graph_objects` and `plotly.express` offer an extensive range of interactive plotting options that enhance user engagement with the data. Plotly is used for creating dynamic visualizations like choropleth maps, interactive scatter plots, and detailed histograms that allow stakeholders to delve deeper into the data insights through zooming, panning, and hovering to display additional data details.\n",
    "\n",
    "- **Squarify**: This library visualizes hierarchical data with adjustable-sized rectangular tree maps, allowing for effective space-filling representations of proportions amongst categories. In the project, Squarify is used to produce treemaps that visually represent the distribution of observations across various categories, such as different countries or educational variables, making it easier to understand complex hierarchical relationships.\n",
    "\n",
    "The combined capabilities of these libraries enable a comprehensive suite of visual tools that support both exploratory data analysis and the presentation of findings in a format conducive to stakeholder understanding and decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d26db4a-4967-4d63-859f-66502f48f594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sas7bdat import SAS7BDAT\n",
    "import pyreadstat\n",
    "import pycountry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3bebfa-68c4-4670-a7ae-31b0c95405a3",
   "metadata": {},
   "source": [
    "## Data Import and Country Code Handling Libraries\n",
    "\n",
    "- **sas7bdat (SAS7BDAT)**: This library is specifically designed for reading SAS data files in the `.sas7bdat` format, which are often used in large-scale data analysis contexts. In our project, `sas7bdat` enables the direct importation of the PISA dataset stored in this format, ensuring that data from such specialized formats is accessible for analysis in Python without the need for conversion.\n",
    "\n",
    "- **pyreadstat**: A library that facilitates the reading and writing of SAS data files along with associated metadata. It provides a convenient bridge to work with `.sas7bdat` files in Python, similarly to how `sas7bdat` is used, but with additional support for reading the metadata, which can be crucial for understanding data structure and contents. This feature is essential for preprocessing steps in the project, as it allows a deeper understanding and manipulation of the data based on its inherent properties.\n",
    "\n",
    "- **pycountry**: Utilized for converting country names and codes between different standards (e.g., ISO, FIPS). In the project, `pycountry` is crucial for mapping country names from the dataset to their corresponding ISO alpha-3 codes. This capability supports the integration of the data with other global datasets and aids in the creation of geographically accurate visualizations such as choropleth maps, enhancing the geographic data analysis aspect of the project.\n",
    "\n",
    "These libraries collectively streamline the data import process from specialized formats and enhance the geographical mapping of data, key aspects that underpin the robust analysis and visualization capabilities of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d27003a-78eb-4863-a469-d9785f940506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c55a906-63cc-49cd-8eed-4b99e6f2e037",
   "metadata": {},
   "source": [
    "## Enhanced Display Handling with IPython\n",
    "\n",
    "- **IPython.display.HTML**: Part of the IPython ecosystem, this library is essential for embedding rich HTML content in Jupyter notebooks. IPython's `HTML` module enables the creation and rendering of HTML content dynamically within notebook cells. This functionality is particularly useful in our project for presenting information in a more visually appealing and interactive format than is possible with plain text output.\n",
    "\n",
    "In this project, `HTML` is used to enhance the presentation of data and analytics results, allowing for the custom formatting of outputs, which can include styling with CSS, embedding images or interactive elements, and more. This capability significantly improves the readability and user interaction with the project's outputs, making it easier for stakeholders to engage with and understand complex data insights directly within the Jupyter notebook environment.\n",
    "\n",
    "The use of `HTML` in IPython effectively bridges the gap between standard data output and a more polished, professional presentation format, catering to both technical and non-technical audiences. This ensures that our data visualizations and results are not only informative but also compelling and accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a71c276-f99d-45b6-a14c-009c2d00bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9747b2f2-5b2a-4b0b-87c9-3c2a3d0dcb3b",
   "metadata": {},
   "source": [
    "## Managing Warnings in Python\n",
    "\n",
    "- **warnings**: This module is a powerful tool for handling warnings in Python programs. In data science projects, particularly those involving multiple libraries that may not always align perfectly in terms of dependencies or deprecated features, managing warnings effectively is crucial to maintain a clean and readable output. Using `warnings.filterwarnings('ignore', category=FutureWarning)` specifically instructs Python to ignore warnings about future changes to the libraries' APIs or deprecated features that have not yet been removed but will be in future versions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25a26891-9542-42b2-8f06-803ab8aa0c25",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HTML' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m html \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m</ul>\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Отображаем HTML\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m display(\u001b[43mHTML\u001b[49m(html))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'HTML' is not defined"
     ]
    }
   ],
   "source": [
    "# file_path = 'cy08msp_stu_qqq.sas7bdat'\n",
    "\n",
    "# columns_to_load = [\n",
    "#    \"CNT\", \"OECD\", \"ST004D01T\", \"ST001D01T\", \"ST126Q01TA\", \"ST125Q01NA\",\n",
    "#    \"PV1MATH\", \"PV2MATH\", \"PV3MATH\", \"PV4MATH\", \"PV5MATH\",\n",
    "#    \"PV1READ\", \"PV2READ\", \"PV3READ\", \"PV4READ\", \"PV5READ\",\n",
    "#    \"PV1SCIE\", \"PV2SCIE\", \"PV3SCIE\", \"PV4SCIE\", \"PV5SCIE\",\n",
    "#    \"ESCS\", \"IMMIG\", \"BELONG\", \"BULLIED\", \"FEELSAFE\",\n",
    "#    \"STUDYHMW\", \"DISCLIM\", \"ADMINMODE\", \"ST019CQ01T\",\n",
    "#    \"ICTHOME\", \"MATHMOT\"\n",
    "#]\n",
    "\n",
    "# df, metadata = pyreadstat.read_sas7bdat(file_path, usecols=columns_to_load)\n",
    "\n",
    "# new_file_path = 'new_processed_pisa_data.csv'\n",
    "# df.to_csv(new_file_path, index=False)\n",
    "\n",
    "# html = '<ul>'\n",
    "# html += ''.join(f'<li>{column}</li>' for column in df.columns)\n",
    "# html += '</ul>'\n",
    "\n",
    "# display(HTML(html))\n",
    "\n",
    "\n",
    "# new_file_path_csv = 'processed_pisa_data.csv'\n",
    "\n",
    "# df.to_csv(new_file_path_csv, index=False)\n",
    "\n",
    "df_first_half = pd.read_csv('first_half.csv')\n",
    "df_second_half = pd.read_csv('second_half.csv')\n",
    "\n",
    "df = pd.concat([df_first_half, df_second_half], ignore_index=True)\n",
    "\n",
    "html = '<ul>'\n",
    "html += ''.join(f'<li>{column}</li>' for column in df.columns)\n",
    "html += '</ul>'\n",
    "\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db6ecd8-93be-4257-854e-f3849130e632",
   "metadata": {},
   "source": [
    "## Correction After Data Reading on Local Machine\n",
    "1. **Loading Data**: \n",
    "   - We used `pyreadstat` to read only the necessary columns from the `cy08msp_stu_qqq.sas7bdat` file. This approach allows us to minimize memory usage and optimize processing time.\n",
    "\n",
    "2. **Data Conversion to CSV**:\n",
    "   - Due to the large size of the `.sas7bdat` file, it is not feasible to upload it directly to GitHub, as it exceeds the platform's file size limitations.\n",
    "   - To address this, we converted the loaded DataFrame into a CSV format using `df.to_csv()`. This conversion significantly reduces the file size, making it manageable and suitable for version control systems like GitHub.\n",
    "\n",
    "3. **Commenting Out Initial Code**:\n",
    "   - The initial lines of code responsible for loading and saving the data are commented out to indicate that these operations were performed prior to uploading the data to GitHub.\n",
    "   - This ensures that anyone reviewing the code understands that the data preparation phase has been completed and the resulting CSV file is ready for analysis and further processing.\n",
    "\n",
    "**Important Note:** The forthcoming descriptions assume that there is no commented-out part of the code. This ensures a clear understanding of our original approach, which involves directly reading from the `.sas7bdat` file into a DataFrame. The explanations and procedures outlined will proceed as if we are interacting with the data freshly loaded from the `.sas7bdat` file, reflecting the steps and methodologies initially used in our data handling process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b15a71-08f7-428c-a973-be26730c074c",
   "metadata": {},
   "source": [
    "## Data Import and Column Selection\n",
    "\n",
    "In this segment of the project, we are focusing on importing a specific dataset and selectively loading certain columns relevant to our analysis. This is executed through the use of the `pyreadstat` library, which provides an interface to read SAS data files directly into Python, preserving the dataset's structure and metadata.\n",
    "\n",
    "### Code Explanation:\n",
    "\n",
    "- **File Path Specification**: The `file_path` variable is defined to store the path of the `.sas7bdat` file. This path points to the location on the local machine where the PISA dataset is stored, making it accessible for loading.\n",
    "\n",
    "- **Columns Selection**: `columns_to_load` is a list containing the names of the specific columns we want to import from the dataset. This selective loading helps optimize memory usage and processing speed by only importing data that is necessary for our subsequent analyses.\n",
    "\n",
    "- **Reading the Dataset**: `pyreadstat.read_sas7bdat()` is called with `file_path` and `usecols` parameters. The `usecols` parameter takes the list `columns_to_load`, which instructs `pyreadstat` to load only those columns specified, enhancing the efficiency of the data import process.\n",
    "\n",
    "- **Generating an HTML List of Columns**: Once the dataset is loaded into the dataframe `df`, we generate an HTML formatted list of the column names. This is done by iterating over `df.columns`, creating an HTML list item for each column name. This not only confirms the columns that have been loaded but also presents this information in a visually appealing format within the Jupyter Notebook.\n",
    "\n",
    "The use of `pyreadstat` here bridges the gap between specialized data storage formats used in large-scale studies like PISA and the flexible, dynamic environment of Python, allowing for sophisticated data manipulation and analysis directly within Jupyter notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7a38d0-1f3b-42bb-88c2-4b87accabd7d",
   "metadata": {},
   "source": [
    "## Selective Variable Inclusion for Analysis\n",
    "\n",
    "Given the extensive number of columns available in the PISA dataset, a key task was to judiciously select a subset of variables from various categories. This selection process aimed to focus on variables that are likely to influence testing outcomes, thus enabling a targeted and insightful analysis. The chosen variables span demographic details, academic performance, socio-cultural factors, educational environment, and technological access, providing a broad spectrum of data for a comprehensive evaluation.\n",
    "\n",
    "### Organized Selection of Variables:\n",
    "\n",
    "#### 1. **Demographic and Country-Specific Information:**\n",
    "- **Country (CNT)**: Essential for identifying educational outcomes across different nations and understanding global educational disparities.\n",
    "- **OECD Membership (OECD)**: Facilitates a comparative analysis of educational systems and outcomes between OECD member countries and others, offering insights into the effectiveness of educational policies within the OECD framework.\n",
    "\n",
    "#### 2. **Student and Family Background:**\n",
    "- **Gender (ST004D01T)**: Critical for assessing gender disparities in educational attainment and supporting gender-inclusive educational strategies.\n",
    "- **Grade (ST001D01T)**: Helps evaluate the impact of academic progression on performance across different educational stages.\n",
    "- **Father's Education Level (ST126Q01TA)** and **Mother's Education Level (ST125Q01NA)**: Serve as indicators of familial socio-economic status, which is a known factor influencing educational access and quality.\n",
    "\n",
    "#### 3. **Academic Performance Scores:**\n",
    "- **Math, Reading, and Science Scores (PV1MATH to PV5SCIE)**: These scores are central to analyzing core academic competencies and are fundamental for educational assessments in PISA.\n",
    "\n",
    "#### 4. **Socio-Cultural and Psychological Factors:**\n",
    "- **Economic Social Cultural Status (ESCS)**: A composite measure that provides a socioeconomic context to student performance, highlighting disparities and opportunities for targeted interventions.\n",
    "- **Immigration Status (IMMIG)**: Crucial for studying the challenges and performances of immigrant students, which are often different from native students.\n",
    "- **Sense of Belonging (BELONG)** and **Feeling of Safety (FEELSAFE)**: Psychological well-being metrics that significantly impact student engagement and academic success.\n",
    "- **Bullying Frequency (BULLIED)**: Reflects the school environment's safety, directly correlating with student's mental health and learning outcomes.\n",
    "\n",
    "#### 5. **Educational Environment and Practices:**\n",
    "- **Weekly Study Time (STUDYHMW)**: Indicates the level of academic engagement outside of school hours, which is predictive of academic success.\n",
    "- **Disciplinary Climate (DISCLIM)**: A measure of classroom management and educational climate, which affects learning efficiency.\n",
    "- **Test Administration Mode (ADMINMODE)**: Distinguishes between digital and paper testing environments, relevant in the context of modern educational practices.\n",
    "\n",
    "#### 6. **Technological Access and Motivation:**\n",
    "- **Access to ICT at Home (ICTHOME)**: Represents the digital divide in education, which has become increasingly important in today's technology-driven world.\n",
    "- **Math Motivation (MATHMOT)**: Directly influences student performance in math and is presumed to also positively impact performance in other subjects, reflecting intrinsic and extrinsic motivations toward academic achievement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2313d51f-90c0-4f20-bb39-85262724407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names_map = {\n",
    "    \"CNT\": \"Country\",\n",
    "    \"OECD\": \"OECD_Membership\",\n",
    "    \"ST004D01T\": \"Gender\",\n",
    "    \"ST001D01T\": \"Grade\",\n",
    "    \"ST126Q01TA\": \"Father_Education_Level\",\n",
    "    \"ST125Q01NA\": \"Mother_Education_Level\",\n",
    "    \"PV1MATH\": \"Math_Score_PV1\",\n",
    "    \"PV2MATH\": \"Math_Score_PV2\",\n",
    "    \"PV3MATH\": \"Math_Score_PV3\",\n",
    "    \"PV4MATH\": \"Math_Score_PV4\",\n",
    "    \"PV5MATH\": \"Math_Score_PV5\",\n",
    "    \"PV1READ\": \"Reading_Score_PV1\",\n",
    "    \"PV2READ\": \"Reading_Score_PV2\",\n",
    "    \"PV3READ\": \"Reading_Score_PV3\",\n",
    "    \"PV4READ\": \"Reading_Score_PV4\",\n",
    "    \"PV5READ\": \"Reading_Score_PV5\",\n",
    "    \"PV1SCIE\": \"Science_Score_PV1\",\n",
    "    \"PV2SCIE\": \"Science_Score_PV2\",\n",
    "    \"PV3SCIE\": \"Science_Score_PV3\",\n",
    "    \"PV4SCIE\": \"Science_Score_PV4\",\n",
    "    \"PV5SCIE\": \"Science_Score_PV5\",\n",
    "    \"ESCS\": \"Economic_Social_Cultural_Status\",\n",
    "    \"IMMIG\": \"Immigration_Status\",\n",
    "    \"BELONG\": \"Sense_of_Belonging\",\n",
    "    \"BULLIED\": \"Bullying_Frequency\",\n",
    "    \"FEELSAFE\": \"Feeling_of_Safety\",\n",
    "    \"STUDYHMW\": \"Weekly_Study_Time\",\n",
    "    \"DISCLIM\": \"Disciplinary_Climate\",\n",
    "    \"ADMINMODE\": \"Test_Administration_Mode\",\n",
    "    \"ST019CQ01T\": \"Student_father’s_country_of_birth\",\n",
    "    \"ICTHOME\": \"Access_to_ICT_at_Home\",\n",
    "    \"MATHMOT\": \"Math_Motivation\"\n",
    "}\n",
    "\n",
    "html = '<ul>'\n",
    "html += ''.join(f'<li>{column}</li>' for column in df.columns)\n",
    "html += '</ul>'\n",
    "\n",
    "# Отображаем HTML\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e974538-e14b-42e3-b7ad-e4818a3de51f",
   "metadata": {},
   "source": [
    "## Data Renaming and Verification\n",
    "\n",
    "In this section of the code, we first map the abbreviated column names from the PISA dataset to more descriptive names using a dictionary called `column_names_map`. This mapping facilitates easier understanding and manipulation of the data by replacing cryptic identifiers with clear, descriptive terms that accurately reflect the content of each column. For example, \"CNT\" is renamed to \"Country\", and \"ST004D01T\" to \"Gender\", making the dataset more intuitive for anyone analyzing the data.\n",
    "\n",
    "After renaming the columns, we generate a formatted HTML list of the new column names to verify that all the renaming operations have been successfully applied. This visual confirmation is crucial to ensure no column has been overlooked and that each name aligns with our analytical framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dd644f-cb5c-4b1e-98f0-b8f5f4f666f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = df.isnull().mean() * 100\n",
    "\n",
    "missing_data_df = pd.DataFrame({'Column': missing_data.index, 'MissingPercentage': missing_data.values})\n",
    "\n",
    "missing_data_df = missing_data_df[missing_data_df['MissingPercentage'] > 0].sort_values('MissingPercentage', ascending=False)\n",
    "\n",
    "fig = px.bar(missing_data_df, x='MissingPercentage', y='Column', orientation='h',\n",
    "             height=800, width=1000,\n",
    "             title='Percentage of Missing Values by Column',\n",
    "             labels={'MissingPercentage': 'Percentage of Missing Values', 'Column': 'Column'},\n",
    "             text='MissingPercentage')\n",
    "\n",
    "fig.update_traces(texttemplate='%{text:.2f}%', textposition='outside', marker_color='rgb(69, 117, 180)')\n",
    "fig.update_layout(plot_bgcolor='rgba(0,0,0,0)', xaxis_showgrid=True, yaxis_showgrid=True,\n",
    "                  paper_bgcolor='rgba(0,0,0,0)', title_x=0.5)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f6d656-939e-4972-9e27-d41397020591",
   "metadata": {},
   "source": [
    "## Handling Missing Data and Visualization\n",
    "\n",
    "In this segment of the code, we focus on identifying and visualizing the percentage of missing values for each column in the dataset. The process begins by calculating the proportion of missing data per column, which is achieved through the `df.isnull().mean() * 100` expression. This expression checks for null values in each column, computes the mean percentage of missing entries, and scales the result to a percentage form for easier interpretation.\n",
    "\n",
    "Once the percentages are calculated, we construct a DataFrame named `missing_data_df` to organize these values along with the corresponding column names. This DataFrame is particularly structured to include only those columns that have missing values, which is filtered by `missing_data_df['MissingPercentage'] > 0`. Furthermore, it is sorted in descending order of missing percentage to prioritize attention to columns with the highest rates of missing data.\n",
    "\n",
    "To visually represent this data, we employ Plotly Express to create a horizontal bar chart. This chart is not only visually appealing but also interactive, allowing for a detailed examination of the data. Each bar represents a column from the dataset, with its length proportional to the percentage of missing values, providing an immediate visual indication of data completeness across the dataset.\n",
    "\n",
    "The visual enhancement of the chart includes:\n",
    "- **Text labels outside bars**: Displaying exact percentages of missing data for clear, quantitative evaluation.\n",
    "- **Color customization**: Using a consistent and distinct color to ensure that the visualization is both aesthetically pleasing and easy to read.\n",
    "- **Background and grid adjustments**: Setting a transparent background and enabling grid lines to focus attention on the data representation itself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1791f7b-5d7b-498e-8fa4-f2651d5520af",
   "metadata": {},
   "source": [
    "The missing data visualization provides critical insights that are fundamental to guiding the subsequent steps in our data preprocessing strategy. for analytical purposes, it's essential to assess the impact of missing data across various features to ensure the reliability and validity of our findings. The analysis reveals a varying degree of missing data across several key variables, which will significantly influence our approach to data preparation and analysis.\n",
    "\n",
    "The most notable is the high percentage of missing values in **Access to ICT at Home (45.97%)**, which implies a substantial data gap in an area crucial for understanding digital literacy's role in educational outcomes. This high level of missing data necessitates careful consideration, such as the potential use of imputation techniques or a sensitivity analysis to understand the impact of missing data on our study's conclusions.\n",
    "\n",
    "Other variables with significant missing data include **Math Motivation (17.47%)** and **Feeling of Safety (15.73%)**. These factors are vital for analyzing student engagement and well-being, which are known to affect academic performance. The absence of this data could bias our analysis, highlighting the importance of addressing these gaps adequately through statistical methods that can handle missingness without compromising the study's overall integrity.\n",
    "\n",
    "Lesser, but still noteworthy percentages in **Disciplinary Climate, Sense of Belonging, Bullying Frequency,** and **Immigration Status** suggest that while these areas are less problematic, they still require attention to ensure comprehensive data coverage.\n",
    "\n",
    "Given these insights, our next steps will involve deciding on the appropriate techniques for handling missing data. Options include imputation where reasonable (particularly where missing data might introduce bias) or possibly excluding variables with excessively high missingness if they are likely to distort the analysis. This strategic approach will help mitigate the risk of drawing inaccurate conclusions and ensure that our analysis remains robust and reliable. The ultimate aim is to maintain the analytical validity while ensuring that our findings are reflective of the true patterns and relationships present in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a6e5eb-acf1-4258-b420-d9c60fe1b633",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
