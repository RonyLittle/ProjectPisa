{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24bdeac4-d42a-46c9-9740-42a9ae74a059",
   "metadata": {},
   "source": [
    "# Deep Analysis of PISA 2022 Data: Interrelations of Academic Achievement with Sociocultural Factors\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This project undertakes an in-depth analysis of the Program for International Student Assessment (PISA), an initiative orchestrated by the Organisation for Economic Co-operation and Development (OECD). PISA is a worldwide study designed to evaluate the educational performance of 15-year-old students and their preparedness for real-world challenges beyond the formal school curriculum. This assessment, which takes place every three years, measures the abilities of students in critical cognitive domains including reading literacy, mathematical literacy, and scientific literacy.\n",
    "\n",
    "The significance of PISA lies in its role as a global benchmark for evaluating education systems worldwide by comparing the skills and knowledge of students across different countries. This comparison helps in identifying effective educational practices and policies. By focusing on how well young adults can apply their knowledge to real-life situations, PISA provides valuable insights into the effectiveness of schooling in different regions and aids in policymaking to enhance educational outcomes.\n",
    "\n",
    "Through rigorous and standardized testing methodologies, PISA evaluates not just rote memorization, but the ability of students to think critically, solve complex problems, and make reasoned decisions. It thus provides a comprehensive picture of students' capabilities in handling the demands of future academic and occupational settings. The insights drawn from PISA data are instrumental for educators, policymakers, and stakeholders in crafting strategies that improve educational standards and foster an environment that nurtures the full potential of every learner.\n",
    "\n",
    "## Data Source\n",
    "\n",
    "For this project, we harness the most recent 2022 data set obtained from the \"Student Questionnaire Data File\" available on the official OECD website. This file is a comprehensive repository of data, meticulously compiled from the responses of students and their parents across various countries. It serves as a foundational element for our analysis, providing both performance scores and a diverse range of background variables. These variables facilitate a deep dive into several critical aspects of the educational landscape:\n",
    "\n",
    "- **Demographic and Socio-economic Profiles**: This category includes detailed demographic information about students and their familial backgrounds, capturing data points such as age, gender, immigration status, as well as the educational levels and socio-economic statuses of their parents. These variables are crucial for understanding the diverse contexts from which students hail and how these factors might influence their educational achievements.\n",
    "\n",
    "- **Educational Performance**: The data file provides scores across fundamental academic subjects—mathematics, reading, and science. These scores are offered as plausible values, which are multiple imputed scores reflecting students’ abilities derived from their test performances. This approach allows for more nuanced statistical analyses and helps in understanding the variability in student performance across different educational systems.\n",
    "\n",
    "- **Learning Environment and Behaviors**: This section sheds light on the non-academic aspects of students' school life, encompassing their study habits, self-reported motivational attitudes, their sense of belonging at their school, and their experiences with bullying. Such data points are invaluable for assessing the psycho-social dimensions that influence educational outcomes.\n",
    "\n",
    "- **Digital Literacy and Resources**: In today’s digital age, access to technology is a significant factor in educational success. This variable assesses the availability and usage of information and communication technologies (ICT) at students’ homes. It examines how these tools are integrated into the learning process and their impact on students' educational performance.\n",
    "\n",
    "By analyzing these detailed and multi-faceted data, this project aims to uncover patterns and trends that offer insights into the factors that most significantly impact student learning outcomes. These insights can help policymakers, educators, and communities to design targeted interventions that enhance educational equity and effectiveness.\n",
    "\n",
    "## Core Analytical and Advanced Programming Methods\n",
    "\n",
    "The project harnesses a diverse array of data analysis techniques tailored specifically to dissect the complex interactions within educational data provided by the PISA 2022 dataset. Here’s a rundown of the primary methods applied:\n",
    "\n",
    "- **Statistical Analysis**:\n",
    "  - **Descriptive Statistics**: Summarize data features like central tendency, variability, and distribution shapes.\n",
    "  - **Shapiro-Wilk Test**: Assess the normality of data distributions, crucial for the validity of many other statistical tests.\n",
    "  - **Correlation Analysis**: Determine the strength and direction of relationships between variables using Pearson’s correlation coefficient.\n",
    "  - **Regression Analysis**: Linear regression to predict educational outcomes and polynomial regression to capture non-linear relationships.\n",
    "\n",
    "- **Machine Learning**:\n",
    "  - **Random Forest Regression**: Utilized to predict outcomes based on multiple input variables and to assess feature importance, providing insights into which factors most significantly impact student performance.\n",
    "  - **Cross-Validation**: Enhance model validation through k-fold cross-validation, ensuring the model’s robustness and generalizability.\n",
    "\n",
    "- **Deep Learning**:\n",
    "  - **Neural Networks**: Deploy neural networks to model complex, non-linear interactions between variables. The multi-layer perceptron architecture facilitates the exploration of deeper patterns in the data, instrumental in uncovering hidden insights.\n",
    "\n",
    "- **Data Visualization**:\n",
    "  - **Plotly and Seaborn**: Generate interactive graphs and static plots to visualize data distributions, correlations, and regression outcomes effectively. This includes creating heatmaps for correlation matrices, scatter plots for regression analysis, and treemaps for hierarchical data exploration.\n",
    "\n",
    "- **Advanced Data Processing**:\n",
    "  - **Handling Missing Data**: Techniques such as imputation and dropping rows/columns to clean the dataset, ensuring the integrity of the analyses.\n",
    "  - **Feature Engineering**: Includes generating polynomial features for regression models and encoding categorical variables to prepare the dataset for machine learning.\n",
    "\n",
    "These methods collectively facilitate a thorough exploration of the intricate dynamics influencing educational achievements across various demographics and socio-economic backgrounds. Through the intelligent application of these techniques, the project aims to provide actionable insights that could inform policy-making and educational strategies.\n",
    "\n",
    "## Project Aim\n",
    "\n",
    "The overarching goal of this project is to dissect and understand the multitude of factors that influence educational outcomes for students assessed in the PISA 2022 survey. By leveraging a comprehensive dataset provided by the OECD, this analysis seeks to uncover the nuanced interplay between students' academic performances and their demographic, socio-economic, and environmental contexts.\n",
    "\n",
    "### Objectives:\n",
    "1. **Identify Key Factors**: Determine the primary demographic, socio-economic, and educational variables that significantly impact students' scores in mathematics, reading, and science.\n",
    "2. **Model Educational Outcomes**: Utilize advanced statistical methods and machine learning algorithms to predict educational outcomes and interpret the relative importance of each predictor. This will include assessing the impact of factors such as access to technology, parental education levels, and school environments on student performance.\n",
    "3. **Evaluate Policy Implications**: Analyze the data to provide evidence-based recommendations to educational authorities and policymakers. The aim is to identify potential areas for intervention that could lead to improvements in educational equity and effectiveness.\n",
    "4. **Promote Educational Equity**: Explore how differences in educational access and quality affect performance across various groups, aiming to highlight disparities and recommend strategies for promoting inclusivity and fairness in education.\n",
    "\n",
    "### Impact:\n",
    "The insights derived from this project are intended to inform and enhance educational policies and practices worldwide. By understanding the factors that drive educational success, stakeholders can implement targeted interventions to support underperforming groups, optimize educational resources, and ultimately raise the standard of education provided to all students. Furthermore, this project aims to stimulate ongoing dialogue among educators, policymakers, and the academic community about how best to harness data-driven insights for educational planning and reform.\n",
    "\n",
    "In summary, this project not only aims to analyze the data from the PISA 2022 survey comprehensively but also seeks to translate these analyses into practical strategies that can lead to real and sustainable improvements in educational systems globally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ae37d5f-e3c9-4f30-a8ae-15e1cd19a8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6cdbc8-3bcb-479f-b36a-9f575888e50e",
   "metadata": {},
   "source": [
    "## Foundational Libraries\n",
    "\n",
    "- **warnings**: Utilized to manage warnings during runtime, particularly for suppressing specific warning categories that might clutter the output, ensuring a clean presentation of results. This is especially useful in a data science context to ignore routine warnings generated by third-party libraries without affecting the interpretation of code execution.\n",
    "\n",
    "- **numpy (np)**: A cornerstone for numerical computing in Python, numpy offers comprehensive support for arrays and matrices alongside a vast library of mathematical functions to operate on these data structures. In our project, it is indispensable for handling numerical operations on arrays efficiently, which underpins various data manipulation tasks.\n",
    "\n",
    "- **pandas (pd)**: Essential for data manipulation and analysis, pandas provides data structures and operations for manipulating numerical tables and time series. This library is crucial in our project for reading, writing, and processing data from various file formats. It enables sophisticated data manipulation capabilities such as merging, reshaping, selecting, as well as robust handling of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d92c2e22-be95-4415-8a6f-86450c3d0779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy.stats import shapiro, randint as sp_randint\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.graphics.regressionplots import add_lowess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ce3468-e5b0-41f9-b93b-cb4af502bed7",
   "metadata": {},
   "source": [
    "## Statistical and Machine Learning Libraries\n",
    "\n",
    "- **scipy.stats, shapiro, sp_randint**: This suite of tools from the SciPy library supports the generation of random variables, conducting statistical tests, and exploratory data analysis. `shapiro` tests for normality, essential for validating assumptions in many statistical models, while `sp_randint` is used for generating discrete random numbers for hyperparameter tuning.\n",
    "\n",
    "- **sklearn.model_selection**: Includes submodules like `train_test_split` for dividing data into training and test sets, `RandomizedSearchCV` for optimizing model parameters through random search, and `cross_val_score` for evaluating a model's performance using cross-validation.\n",
    "\n",
    "- **sklearn.preprocessing**: Contains `PolynomialFeatures` for generating polynomial and interaction features and `StandardScaler` for feature scaling by standardizing variables. These preprocessing tools are vital for enhancing model performance and accuracy.\n",
    "\n",
    "- **sklearn.linear_model**: Provides models like `LinearRegression` for standard linear regression analysis and `SGDRegressor` for linear models fitted by stochastic gradient descent, a practical approach for large datasets.\n",
    "\n",
    "- **sklearn.ensemble**: Features `RandomForestRegressor`, an ensemble method based on randomized decision trees, known for its high accuracy and robustness against overfitting, particularly useful for regression tasks in complex datasets.\n",
    "\n",
    "- **sklearn.metrics**: Includes performance metrics such as `mean_squared_error` for quantifying the accuracy of regression models and `r2_score` for assessing the proportion of variance captured by the model.\n",
    "\n",
    "- **statsmodels.api**: Offers classes and functions for the estimation of different statistical models, as well as for conducting statistical tests and data exploration. An essential tool for in-depth statistical analysis, often used for regression diagnostics, time-series analysis, and hypothesis testing.\n",
    "\n",
    "- **statsmodels' diagnostic tools**: `het_breuschpagan` tests for heteroscedasticity, `durbin_watson` assesses autocorrelation in residuals from a regression, `variance_inflation_factor` evaluates multicollinearity, and `add_lowess` (locally weighted scatterplot smoothing) is useful for trend fitting in regression diagnostics. These tools provide deeper insights into the model's assumptions and performance, ensuring robust statistical inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "015bb99c-a8b6-4404-ae8c-bccba480c589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2c6cd5-26e0-4742-ac89-4460ada1630e",
   "metadata": {},
   "source": [
    "## Deep Learning Frameworks: TensorFlow and Keras\n",
    "\n",
    "- **TensorFlow**: An open-source library developed by Google for numerical computation and machine learning. TensorFlow provides a broad toolkit for developing and training machine learning models, including powerful features for deep learning. It is used in this project to harness complex patterns in the data that simpler models might miss, especially beneficial for large datasets with intricate features.\n",
    "\n",
    "- **tensorflow.keras.models**: Contains `Sequential`, which is a linear stack of layers used for creating models. The Sequential model is straightforward to understand and use, which is particularly useful for standard deep learning applications where layers are added in sequence.\n",
    "\n",
    "- **tensorflow.keras.layers**: Provides various layers, including `Dense`, which is a regular densely-connected neural network layer. Dense layers are fundamental in neural networks for learning high-level patterns in large data sets and are used extensively in our models to process and learn from educational data. Each neuron in a Dense layer receives input from all neurons of the previous layer, thus being well-suited for pattern recognition tasks found in complex datasets like PISA.\n",
    "\n",
    "These tools are integral to building neural network architectures, facilitating the exploration and implementation of deep learning models that can potentially reveal nonlinear relationships and interactions not detectable by traditional statistical methods. This capability is particularly valuable in educational research, where interactions between variables are complex and multidimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a53dc3f-ccf4-4c29-ac1a-1f5ddb44b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import squarify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a576052-a06e-4620-946d-2ee6ed74f824",
   "metadata": {},
   "source": [
    "## Data Visualization Libraries\n",
    "\n",
    "- **Matplotlib (plt)**: A powerful plotting library for Python, Matplotlib is fundamental for creating static, interactive, and animated visualizations in Python. In this project, Matplotlib is utilized primarily for generating histograms, scatter plots, and more complex visualizations like residual plots, providing a traditional and detailed approach to data visualization.\n",
    "\n",
    "- **Seaborn (sns)**: Built on top of Matplotlib, Seaborn extends its functionality, making it easier to generate complex visualizations with more attractive and informative statistical graphics. This library is used for creating enhanced visualizations such as heatmaps for correlation matrices, which are crucial for identifying relationships between variables in the dataset.\n",
    "\n",
    "- **Plotly (go, px, ff)**: A modern platform for creating interactive plots and dashboards. Plotly's Python graphing libraries `plotly.graph_objects` and `plotly.express` offer an extensive range of interactive plotting options that enhance user engagement with the data. Plotly is used for creating dynamic visualizations like choropleth maps, interactive scatter plots, and detailed histograms that allow stakeholders to delve deeper into the data insights through zooming, panning, and hovering to display additional data details.\n",
    "\n",
    "- **Squarify**: This library visualizes hierarchical data with adjustable-sized rectangular tree maps, allowing for effective space-filling representations of proportions amongst categories. In the project, Squarify is used to produce treemaps that visually represent the distribution of observations across various categories, such as different countries or educational variables, making it easier to understand complex hierarchical relationships.\n",
    "\n",
    "The combined capabilities of these libraries enable a comprehensive suite of visual tools that support both exploratory data analysis and the presentation of findings in a format conducive to stakeholder understanding and decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d26db4a-4967-4d63-859f-66502f48f594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sas7bdat import SAS7BDAT\n",
    "import pyreadstat\n",
    "import pycountry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3bebfa-68c4-4670-a7ae-31b0c95405a3",
   "metadata": {},
   "source": [
    "## Data Import and Country Code Handling Libraries\n",
    "\n",
    "- **sas7bdat (SAS7BDAT)**: This library is specifically designed for reading SAS data files in the `.sas7bdat` format, which are often used in large-scale data analysis contexts. In our project, `sas7bdat` enables the direct importation of the PISA dataset stored in this format, ensuring that data from such specialized formats is accessible for analysis in Python without the need for conversion.\n",
    "\n",
    "- **pyreadstat**: A library that facilitates the reading and writing of SAS data files along with associated metadata. It provides a convenient bridge to work with `.sas7bdat` files in Python, similarly to how `sas7bdat` is used, but with additional support for reading the metadata, which can be crucial for understanding data structure and contents. This feature is essential for preprocessing steps in the project, as it allows a deeper understanding and manipulation of the data based on its inherent properties.\n",
    "\n",
    "- **pycountry**: Utilized for converting country names and codes between different standards (e.g., ISO, FIPS). In the project, `pycountry` is crucial for mapping country names from the dataset to their corresponding ISO alpha-3 codes. This capability supports the integration of the data with other global datasets and aids in the creation of geographically accurate visualizations such as choropleth maps, enhancing the geographic data analysis aspect of the project.\n",
    "\n",
    "These libraries collectively streamline the data import process from specialized formats and enhance the geographical mapping of data, key aspects that underpin the robust analysis and visualization capabilities of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d27003a-78eb-4863-a469-d9785f940506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c55a906-63cc-49cd-8eed-4b99e6f2e037",
   "metadata": {},
   "source": [
    "## Enhanced Display Handling with IPython\n",
    "\n",
    "- **IPython.display.HTML**: Part of the IPython ecosystem, this library is essential for embedding rich HTML content in Jupyter notebooks. IPython's `HTML` module enables the creation and rendering of HTML content dynamically within notebook cells. This functionality is particularly useful in our project for presenting information in a more visually appealing and interactive format than is possible with plain text output.\n",
    "\n",
    "In this project, `HTML` is used to enhance the presentation of data and analytics results, allowing for the custom formatting of outputs, which can include styling with CSS, embedding images or interactive elements, and more. This capability significantly improves the readability and user interaction with the project's outputs, making it easier for stakeholders to engage with and understand complex data insights directly within the Jupyter notebook environment.\n",
    "\n",
    "The use of `HTML` in IPython effectively bridges the gap between standard data output and a more polished, professional presentation format, catering to both technical and non-technical audiences. This ensures that our data visualizations and results are not only informative but also compelling and accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a71c276-f99d-45b6-a14c-009c2d00bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9747b2f2-5b2a-4b0b-87c9-3c2a3d0dcb3b",
   "metadata": {},
   "source": [
    "## Managing Warnings in Python\n",
    "\n",
    "- **warnings**: This module is a powerful tool for handling warnings in Python programs. In data science projects, particularly those involving multiple libraries that may not always align perfectly in terms of dependencies or deprecated features, managing warnings effectively is crucial to maintain a clean and readable output. Using `warnings.filterwarnings('ignore', category=FutureWarning)` specifically instructs Python to ignore warnings about future changes to the libraries' APIs or deprecated features that have not yet been removed but will be in future versions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25a26891-9542-42b2-8f06-803ab8aa0c25",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HTML' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m html \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m</ul>\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Отображаем HTML\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m display(\u001b[43mHTML\u001b[49m(html))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'HTML' is not defined"
     ]
    }
   ],
   "source": [
    "# file_path = 'cy08msp_stu_qqq.sas7bdat'\n",
    "\n",
    "# columns_to_load = [\n",
    "#    \"CNT\", \"OECD\", \"ST004D01T\", \"ST001D01T\", \"ST126Q01TA\", \"ST125Q01NA\",\n",
    "#    \"PV1MATH\", \"PV2MATH\", \"PV3MATH\", \"PV4MATH\", \"PV5MATH\",\n",
    "#    \"PV1READ\", \"PV2READ\", \"PV3READ\", \"PV4READ\", \"PV5READ\",\n",
    "#    \"PV1SCIE\", \"PV2SCIE\", \"PV3SCIE\", \"PV4SCIE\", \"PV5SCIE\",\n",
    "#    \"ESCS\", \"IMMIG\", \"BELONG\", \"BULLIED\", \"FEELSAFE\",\n",
    "#    \"STUDYHMW\", \"DISCLIM\", \"ADMINMODE\", \"ST019CQ01T\",\n",
    "#    \"ICTHOME\", \"MATHMOT\"\n",
    "#]\n",
    "\n",
    "# df, metadata = pyreadstat.read_sas7bdat(file_path, usecols=columns_to_load)\n",
    "\n",
    "# new_file_path = 'new_processed_pisa_data.csv'\n",
    "# df.to_csv(new_file_path, index=False)\n",
    "\n",
    "# html = '<ul>'\n",
    "# html += ''.join(f'<li>{column}</li>' for column in df.columns)\n",
    "# html += '</ul>'\n",
    "\n",
    "# display(HTML(html))\n",
    "\n",
    "\n",
    "# new_file_path_csv = 'processed_pisa_data.csv'\n",
    "\n",
    "# df.to_csv(new_file_path_csv, index=False)\n",
    "\n",
    "df_first_half = pd.read_csv('first_half.csv')\n",
    "df_second_half = pd.read_csv('second_half.csv')\n",
    "\n",
    "df = pd.concat([df_first_half, df_second_half], ignore_index=True)\n",
    "\n",
    "html = '<ul>'\n",
    "html += ''.join(f'<li>{column}</li>' for column in df.columns)\n",
    "html += '</ul>'\n",
    "\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7214c6ef-8f7d-47da-8845-2007f6b9bbc5",
   "metadata": {},
   "source": [
    "## Correction After Data Reading on Local Machine\n",
    "1. **Loading Data**: \n",
    "   - We used `pyreadstat` to read only the necessary columns from the `cy08msp_stu_qqq.sas7bdat` file. This approach allows us to minimize memory usage and optimize processing time.\n",
    "\n",
    "2. **Data Conversion to CSV**:\n",
    "   - Due to the large size of the `.sas7bdat` file, it is not feasible to upload it directly to GitHub, as it exceeds the platform's file size limitations.\n",
    "   - To address this, we converted the loaded DataFrame into a CSV format using `df.to_csv()`. This conversion significantly reduces the file size, making it manageable and suitable for version control systems like GitHub.\n",
    "\n",
    "3. **Commenting Out Initial Code**:\n",
    "   - The initial lines of code responsible for loading and saving the data are commented out to indicate that these operations were performed prior to uploading the data to GitHub.\n",
    "   - This ensures that anyone reviewing the code understands that the data preparation phase has been completed and the resulting CSV file is ready for analysis and further processing.\n",
    "\n",
    "**Important Note:** The forthcoming descriptions assume that there is no commented-out part of the code. This ensures a clear understanding of our original approach, which involves directly reading from the `.sas7bdat` file into a DataFrame. The explanations and procedures outlined will proceed as if we are interacting with the data freshly loaded from the `.sas7bdat` file, reflecting the steps and methodologies initially used in our data handling process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b15a71-08f7-428c-a973-be26730c074c",
   "metadata": {},
   "source": [
    "## Data Import and Column Selection\n",
    "\n",
    "In this segment of the project, we are focusing on importing a specific dataset and selectively loading certain columns relevant to our analysis. This is executed through the use of the `pyreadstat` library, which provides an interface to read SAS data files directly into Python, preserving the dataset's structure and metadata.\n",
    "\n",
    "### Code Explanation:\n",
    "\n",
    "- **File Path Specification**: The `file_path` variable is defined to store the path of the `.sas7bdat` file. This path points to the location on the local machine where the PISA dataset is stored, making it accessible for loading.\n",
    "\n",
    "- **Columns Selection**: `columns_to_load` is a list containing the names of the specific columns we want to import from the dataset. This selective loading helps optimize memory usage and processing speed by only importing data that is necessary for our subsequent analyses.\n",
    "\n",
    "- **Reading the Dataset**: `pyreadstat.read_sas7bdat()` is called with `file_path` and `usecols` parameters. The `usecols` parameter takes the list `columns_to_load`, which instructs `pyreadstat` to load only those columns specified, enhancing the efficiency of the data import process.\n",
    "\n",
    "- **Generating an HTML List of Columns**: Once the dataset is loaded into the dataframe `df`, we generate an HTML formatted list of the column names. This is done by iterating over `df.columns`, creating an HTML list item for each column name. This not only confirms the columns that have been loaded but also presents this information in a visually appealing format within the Jupyter Notebook.\n",
    "\n",
    "The use of `pyreadstat` here bridges the gap between specialized data storage formats used in large-scale studies like PISA and the flexible, dynamic environment of Python, allowing for sophisticated data manipulation and analysis directly within Jupyter notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7a38d0-1f3b-42bb-88c2-4b87accabd7d",
   "metadata": {},
   "source": [
    "## Selective Variable Inclusion for Analysis\n",
    "\n",
    "Given the extensive number of columns available in the PISA dataset, a key task was to judiciously select a subset of variables from various categories. This selection process aimed to focus on variables that are likely to influence testing outcomes, thus enabling a targeted and insightful analysis. The chosen variables span demographic details, academic performance, socio-cultural factors, educational environment, and technological access, providing a broad spectrum of data for a comprehensive evaluation.\n",
    "\n",
    "### Organized Selection of Variables:\n",
    "\n",
    "#### 1. **Demographic and Country-Specific Information:**\n",
    "- **Country (CNT)**: Essential for identifying educational outcomes across different nations and understanding global educational disparities.\n",
    "- **OECD Membership (OECD)**: Facilitates a comparative analysis of educational systems and outcomes between OECD member countries and others, offering insights into the effectiveness of educational policies within the OECD framework.\n",
    "\n",
    "#### 2. **Student and Family Background:**\n",
    "- **Gender (ST004D01T)**: Critical for assessing gender disparities in educational attainment and supporting gender-inclusive educational strategies.\n",
    "- **Grade (ST001D01T)**: Helps evaluate the impact of academic progression on performance across different educational stages.\n",
    "- **Father's Education Level (ST126Q01TA)** and **Mother's Education Level (ST125Q01NA)**: Serve as indicators of familial socio-economic status, which is a known factor influencing educational access and quality.\n",
    "\n",
    "#### 3. **Academic Performance Scores:**\n",
    "- **Math, Reading, and Science Scores (PV1MATH to PV5SCIE)**: These scores are central to analyzing core academic competencies and are fundamental for educational assessments in PISA.\n",
    "\n",
    "#### 4. **Socio-Cultural and Psychological Factors:**\n",
    "- **Economic Social Cultural Status (ESCS)**: A composite measure that provides a socioeconomic context to student performance, highlighting disparities and opportunities for targeted interventions.\n",
    "- **Immigration Status (IMMIG)**: Crucial for studying the challenges and performances of immigrant students, which are often different from native students.\n",
    "- **Sense of Belonging (BELONG)** and **Feeling of Safety (FEELSAFE)**: Psychological well-being metrics that significantly impact student engagement and academic success.\n",
    "- **Bullying Frequency (BULLIED)**: Reflects the school environment's safety, directly correlating with student's mental health and learning outcomes.\n",
    "\n",
    "#### 5. **Educational Environment and Practices:**\n",
    "- **Weekly Study Time (STUDYHMW)**: Indicates the level of academic engagement outside of school hours, which is predictive of academic success.\n",
    "- **Disciplinary Climate (DISCLIM)**: A measure of classroom management and educational climate, which affects learning efficiency.\n",
    "- **Test Administration Mode (ADMINMODE)**: Distinguishes between digital and paper testing environments, relevant in the context of modern educational practices.\n",
    "\n",
    "#### 6. **Technological Access and Motivation:**\n",
    "- **Access to ICT at Home (ICTHOME)**: Represents the digital divide in education, which has become increasingly important in today's technology-driven world.\n",
    "- **Math Motivation (MATHMOT)**: Directly influences student performance in math and is presumed to also positively impact performance in other subjects, reflecting intrinsic and extrinsic motivations toward academic achievement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2313d51f-90c0-4f20-bb39-85262724407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names_map = {\n",
    "    \"CNT\": \"Country\",\n",
    "    \"OECD\": \"OECD_Membership\",\n",
    "    \"ST004D01T\": \"Gender\",\n",
    "    \"ST001D01T\": \"Grade\",\n",
    "    \"ST126Q01TA\": \"Father_Education_Level\",\n",
    "    \"ST125Q01NA\": \"Mother_Education_Level\",\n",
    "    \"PV1MATH\": \"Math_Score_PV1\",\n",
    "    \"PV2MATH\": \"Math_Score_PV2\",\n",
    "    \"PV3MATH\": \"Math_Score_PV3\",\n",
    "    \"PV4MATH\": \"Math_Score_PV4\",\n",
    "    \"PV5MATH\": \"Math_Score_PV5\",\n",
    "    \"PV1READ\": \"Reading_Score_PV1\",\n",
    "    \"PV2READ\": \"Reading_Score_PV2\",\n",
    "    \"PV3READ\": \"Reading_Score_PV3\",\n",
    "    \"PV4READ\": \"Reading_Score_PV4\",\n",
    "    \"PV5READ\": \"Reading_Score_PV5\",\n",
    "    \"PV1SCIE\": \"Science_Score_PV1\",\n",
    "    \"PV2SCIE\": \"Science_Score_PV2\",\n",
    "    \"PV3SCIE\": \"Science_Score_PV3\",\n",
    "    \"PV4SCIE\": \"Science_Score_PV4\",\n",
    "    \"PV5SCIE\": \"Science_Score_PV5\",\n",
    "    \"ESCS\": \"Economic_Social_Cultural_Status\",\n",
    "    \"IMMIG\": \"Immigration_Status\",\n",
    "    \"BELONG\": \"Sense_of_Belonging\",\n",
    "    \"BULLIED\": \"Bullying_Frequency\",\n",
    "    \"FEELSAFE\": \"Feeling_of_Safety\",\n",
    "    \"STUDYHMW\": \"Weekly_Study_Time\",\n",
    "    \"DISCLIM\": \"Disciplinary_Climate\",\n",
    "    \"ADMINMODE\": \"Test_Administration_Mode\",\n",
    "    \"ST019CQ01T\": \"Student_father’s_country_of_birth\",\n",
    "    \"ICTHOME\": \"Access_to_ICT_at_Home\",\n",
    "    \"MATHMOT\": \"Math_Motivation\"\n",
    "}\n",
    "\n",
    "html = '<ul>'\n",
    "html += ''.join(f'<li>{column}</li>' for column in df.columns)\n",
    "html += '</ul>'\n",
    "\n",
    "# Отображаем HTML\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e974538-e14b-42e3-b7ad-e4818a3de51f",
   "metadata": {},
   "source": [
    "## Data Renaming and Verification\n",
    "\n",
    "In this section of the code, we first map the abbreviated column names from the PISA dataset to more descriptive names using a dictionary called `column_names_map`. This mapping facilitates easier understanding and manipulation of the data by replacing cryptic identifiers with clear, descriptive terms that accurately reflect the content of each column. For example, \"CNT\" is renamed to \"Country\", and \"ST004D01T\" to \"Gender\", making the dataset more intuitive for anyone analyzing the data.\n",
    "\n",
    "After renaming the columns, we generate a formatted HTML list of the new column names to verify that all the renaming operations have been successfully applied. This visual confirmation is crucial to ensure no column has been overlooked and that each name aligns with our analytical framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dd644f-cb5c-4b1e-98f0-b8f5f4f666f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = df.isnull().mean() * 100\n",
    "\n",
    "missing_data_df = pd.DataFrame({'Column': missing_data.index, 'MissingPercentage': missing_data.values})\n",
    "\n",
    "missing_data_df = missing_data_df[missing_data_df['MissingPercentage'] > 0].sort_values('MissingPercentage', ascending=False)\n",
    "\n",
    "fig = px.bar(missing_data_df, x='MissingPercentage', y='Column', orientation='h',\n",
    "             height=800, width=1000,\n",
    "             title='Percentage of Missing Values by Column',\n",
    "             labels={'MissingPercentage': 'Percentage of Missing Values', 'Column': 'Column'},\n",
    "             text='MissingPercentage')\n",
    "\n",
    "fig.update_traces(texttemplate='%{text:.2f}%', textposition='outside', marker_color='rgb(69, 117, 180)')\n",
    "fig.update_layout(plot_bgcolor='rgba(0,0,0,0)', xaxis_showgrid=True, yaxis_showgrid=True,\n",
    "                  paper_bgcolor='rgba(0,0,0,0)', title_x=0.5)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f6d656-939e-4972-9e27-d41397020591",
   "metadata": {},
   "source": [
    "## Handling Missing Data and Visualization\n",
    "\n",
    "In this segment of the code, we focus on identifying and visualizing the percentage of missing values for each column in the dataset. The process begins by calculating the proportion of missing data per column, which is achieved through the `df.isnull().mean() * 100` expression. This expression checks for null values in each column, computes the mean percentage of missing entries, and scales the result to a percentage form for easier interpretation.\n",
    "\n",
    "Once the percentages are calculated, we construct a DataFrame named `missing_data_df` to organize these values along with the corresponding column names. This DataFrame is particularly structured to include only those columns that have missing values, which is filtered by `missing_data_df['MissingPercentage'] > 0`. Furthermore, it is sorted in descending order of missing percentage to prioritize attention to columns with the highest rates of missing data.\n",
    "\n",
    "To visually represent this data, we employ Plotly Express to create a horizontal bar chart. This chart is not only visually appealing but also interactive, allowing for a detailed examination of the data. Each bar represents a column from the dataset, with its length proportional to the percentage of missing values, providing an immediate visual indication of data completeness across the dataset.\n",
    "\n",
    "The visual enhancement of the chart includes:\n",
    "- **Text labels outside bars**: Displaying exact percentages of missing data for clear, quantitative evaluation.\n",
    "- **Color customization**: Using a consistent and distinct color to ensure that the visualization is both aesthetically pleasing and easy to read.\n",
    "- **Background and grid adjustments**: Setting a transparent background and enabling grid lines to focus attention on the data representation itself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1791f7b-5d7b-498e-8fa4-f2651d5520af",
   "metadata": {},
   "source": [
    "The missing data visualization provides critical insights that are fundamental to guiding the subsequent steps in our data preprocessing strategy. for analytical purposes, it's essential to assess the impact of missing data across various features to ensure the reliability and validity of our findings. The analysis reveals a varying degree of missing data across several key variables, which will significantly influence our approach to data preparation and analysis.\n",
    "\n",
    "The most notable is the high percentage of missing values in **Access to ICT at Home (45.97%)**, which implies a substantial data gap in an area crucial for understanding digital literacy's role in educational outcomes. This high level of missing data necessitates careful consideration, such as the potential use of imputation techniques or a sensitivity analysis to understand the impact of missing data on our study's conclusions.\n",
    "\n",
    "Other variables with significant missing data include **Math Motivation (17.47%)** and **Feeling of Safety (15.73%)**. These factors are vital for analyzing student engagement and well-being, which are known to affect academic performance. The absence of this data could bias our analysis, highlighting the importance of addressing these gaps adequately through statistical methods that can handle missingness without compromising the study's overall integrity.\n",
    "\n",
    "Lesser, but still noteworthy percentages in **Disciplinary Climate, Sense of Belonging, Bullying Frequency,** and **Immigration Status** suggest that while these areas are less problematic, they still require attention to ensure comprehensive data coverage.\n",
    "\n",
    "Given these insights, our next steps will involve deciding on the appropriate techniques for handling missing data. Options include imputation where reasonable (particularly where missing data might introduce bias) or possibly excluding variables with excessively high missingness if they are likely to distort the analysis. This strategic approach will help mitigate the risk of drawing inaccurate conclusions and ensure that our analysis remains robust and reliable. The ultimate aim is to maintain the analytical validity while ensuring that our findings are reflective of the true patterns and relationships present in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a6e5eb-acf1-4258-b420-d9c60fe1b633",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantitative_vars = ['Access_to_ICT_at_Home', 'Feeling_of_Safety']\n",
    "for var in quantitative_vars:\n",
    "    median_value = df[var].median()\n",
    "    df[var].fillna(median_value, inplace=True)\n",
    "\n",
    "categorical_vars = ['Math_Motivation']\n",
    "for var in categorical_vars:\n",
    "    mode_value = df[var].mode()[0]\n",
    "    df[var].fillna(mode_value, inplace=True)\n",
    "\n",
    "low_missing_vars = ['Disciplinary_Climate', 'Sense_of_Belonging', 'Bullying_Frequency', 'Immigration_Status', \n",
    "                    'Father_Education_Level', 'Weekly_Study_Time', 'Mother_Education_Level', 'Student_father’s_country_of_birth',\n",
    "                    'Economic_Social_Cultural_Status', 'Grade', 'Gender']\n",
    "df.dropna(subset=low_missing_vars, inplace=True)\n",
    "\n",
    "missing_values_summary = df.isnull().sum()\n",
    "\n",
    "print(\"Summary of missing values for each column:\")\n",
    "print(missing_values_summary[missing_values_summary > 0])\n",
    "\n",
    "if df.isnull().any().any():\n",
    "    print(\"There are still missing values in the dataframe.\")\n",
    "else:\n",
    "    print(\"No missing values found in the dataframe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20701b7e-b0a2-44c6-83a3-0dd16f9d6298",
   "metadata": {},
   "source": [
    "### Filling Missing Values for Quantitative Variables\n",
    "\n",
    "We used pandas' `median()` function to compute the median of 'Access_to_ICT_at_Home' and 'Feeling_of_Safety', followed by `fillna()` to apply these median values where data was missing. This is done in a loop over each quantitative variable to automate the process and reduce the risk of errors.\n",
    "\n",
    "### Imputation for a Categorical Variable\n",
    "\n",
    "For 'Math_Motivation', the mode (most common value) was determined using pandas' `mode()` function and then used to fill missing entries. This method ensures that the imputed values are representative of the most frequently occurring category within the dataset.\n",
    "\n",
    "### Dropping Rows with Few Missing Values\n",
    "\n",
    "We identified specific columns with minimal missing values and decided to remove any rows with missing data in these columns. This was achieved using `dropna()` with the subset parameter, which specifies the columns to consider for dropping rows. This approach cleans the data while retaining as much information as possible.\n",
    "\n",
    "### Final Verification\n",
    "\n",
    "After all imputation steps, we ran a check using `isnull().sum()` to list any columns still containing missing values and `any().any()` to confirm the absence of missing data. This verification step is crucial to ensure data integrity before moving forward with any further analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4500f50-beb5-4bfb-8447-4c0860ec9f8f",
   "metadata": {},
   "source": [
    "## Comprehensive Handling of Missing Data\n",
    "\n",
    "In our approach to managing missing data, we meticulously categorized variables based on their nature and proportion of missing values to apply the most suitable imputation techniques, enhancing the dataset's completeness without compromising its integrity.\n",
    "\n",
    "### Handling Quantitative Variables\n",
    "\n",
    "For quantitative variables such as 'Access_to_ICT_at_Home' and 'Feeling_of_Safety', which had significant percentages of missing data, we used median imputation. The median is less affected by outliers and skewed data, making it an appropriate choice for filling gaps in these numerical columns. This approach helps maintain the distributional characteristics of the original data.\n",
    "\n",
    "### Handling a Categorical Variable\n",
    "\n",
    "The variable 'Math_Motivation' is categorical and had missing entries. We filled these gaps with the mode of the column, which is the most frequent value. Using the mode for categorical data ensures that the imputed values align with the most commonly observed category, preserving the dataset's underlying distributions without introducing bias.\n",
    "\n",
    "### Dealing with Variables Having Few Missing Values\n",
    "\n",
    "For columns like 'Disciplinary_Climate', 'Sense_of_Belonging', and others with relatively low missing values, we opted for row deletion. This method was chosen because the low percentage of missing data meant that removing these rows would not lead to significant loss of information but would ensure a completely clean dataset for analysis.\n",
    "\n",
    "### Verification of Imputation Effectiveness\n",
    "\n",
    "Post-imputation, we performed a thorough check to ensure no remaining missing values. The results confirmed that our strategies effectively addressed all gaps, as evidenced by the absence of missing data across all considered columns.\n",
    "\n",
    "This careful handling of missing data ensures that our dataset is primed for high-quality analyses, reflecting an accurate representation of the underlying phenomena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7a00be-a47e-4e62-a1b6-cc63628727de",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_counts = df['Country'].value_counts()\n",
    "total_observations = country_counts.sum()\n",
    "country_percentage = (country_counts / total_observations) * 100\n",
    "country_data = pd.DataFrame({\n",
    "    'Country': country_counts.index,\n",
    "    'Observations': country_counts.values,\n",
    "    'Percentage': country_percentage.values\n",
    "})\n",
    "\n",
    "fig = px.treemap(country_data, path=['Country'], values='Observations',\n",
    "                 color='Percentage', hover_data=['Observations', 'Percentage'],\n",
    "                 color_continuous_scale='RdYlGn', title='Distribution of Observations by Country')\n",
    "\n",
    "fig.update_layout(margin=dict(t=50, l=25, r=25, b=25))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0bd1e3-e71e-4074-b0db-2233529ceb88",
   "metadata": {},
   "source": [
    "## Data Preparation and Visualization: Country Distribution\n",
    "\n",
    "In this section, we delve into the geographic distribution of the dataset's observations by examining the frequency of records from different countries. This analysis is pivotal for understanding the data's breadth and ensuring a balanced representation across various regions.\n",
    "\n",
    "### Step-by-Step Code Explanation:\n",
    "\n",
    "- **Counting Observations per Country**: We use `df['Country'].value_counts()` to generate a series containing the counts of occurrences for each country in the dataset. This method efficiently tallies the number of rows for each unique country, providing a clear view of the dataset's composition.\n",
    "\n",
    "- **Calculating Total Observations**: The total number of observations within the dataset is calculated by summing up all entries in the `country_counts` series using `country_counts.sum()`. This total serves as the basis for further calculations, like percentage distribution.\n",
    "\n",
    "- **Percentage Calculation**: To put the data into perspective relative to the entire dataset, we compute the percentage of observations for each country with `country_counts / total_observations * 100`. This gives a normalized view of the data, highlighting the proportionate contribution of each country.\n",
    "\n",
    "- **Creating DataFrame for Visualization**: We then construct a DataFrame named `country_data` that contains columns for `Country`, `Observations`, and `Percentage`. This structured format is necessary for generating visualizations that require specific data arrangements, such as treemaps.\n",
    "\n",
    "- **Generating a Treemap**: Using Plotly Express, we create an interactive treemap with `px.treemap()`. The treemap displays each country as a distinct segment, sized and colored based on the number of observations. We configure the treemap with hover data to show detailed statistics when a segment is hovered over. This visualization method is particularly effective for illustrating hierarchical (nested) data and provides a quick, digestible format to assess the data's geographical distribution.\n",
    "\n",
    "- **Layout Customization**: The layout of the treemap is customized to enhance readability and aesthetics, adjusting margins and title positioning to fit the presentation context better.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce97622-4e9e-475a-a8c1-1b91cc85975f",
   "metadata": {},
   "source": [
    "## Analysis of Country-Wise Observations in PISA Dataset\n",
    "\n",
    "The data exploration stage of our project includes a comprehensive visualization of the distribution of observations by country within the PISA dataset. This analysis is pivotal as it highlights the breadth and diversity of data available across different regions, which is essential for ensuring the robustness of our subsequent analyses.\n",
    "\n",
    "### Interpretation of the Treemap Visualization:\n",
    "\n",
    "- **Top Observations**: The countries with the highest number of observations, such as Spain (ESP), United Arab Emirates (ARE), and Kazakhstan (KAZ), indicate extensive data collection efforts in these regions. Spain leads with 5.74% of the total observations, suggesting a very active participation in the PISA survey.\n",
    "\n",
    "- **Significant Contributors**: Countries like Canada (CAN), Indonesia (IDN), and Australia (AUS) also contribute significantly, with more than 2% of the total observations each. These contributions are crucial as they ensure a diverse input reflective of various educational systems.\n",
    "\n",
    "- **Moderate to Low Observations**: Countries like Italy, the United Kingdom, and Finland show moderate levels of participation. It’s notable that countries with historically strong educational outcomes like Finland still contribute valuable data (1.85%).\n",
    "\n",
    "- **Global Participation**: The data includes countries from varying socio-economic backgrounds and geographic locations, from Argentina in South America to Vietnam in Asia, providing a global perspective on educational achievement.\n",
    "\n",
    "- **Special Cases**: Smaller countries or regions like Malta (MLT), Iceland (ISL), and Panama (PAN) show lower participation rates, which is expected due to their smaller population sizes.\n",
    "\n",
    "### Analytical Insights:\n",
    "\n",
    "- **Global Coverage and Educational Insights**: The widespread coverage from 75 countries provides a robust dataset that is invaluable for cross-country comparisons and understanding global educational trends.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229aca6f-2424-4248-ba02-69a892eec059",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_scores = df.groupby('Country')[['Math_Score_PV1', 'Math_Score_PV2', 'Math_Score_PV3', 'Math_Score_PV4', 'Math_Score_PV5']].mean().mean(axis=1).round(0)\n",
    "\n",
    "reading_scores = df.groupby('Country')[['Reading_Score_PV1', 'Reading_Score_PV2', 'Reading_Score_PV3', 'Reading_Score_PV4', 'Reading_Score_PV5']].mean().mean(axis=1).round(0)\n",
    "\n",
    "science_scores = df.groupby('Country')[['Science_Score_PV1', 'Science_Score_PV2', 'Science_Score_PV3', 'Science_Score_PV4', 'Science_Score_PV5']].mean().mean(axis=1).round(0)\n",
    "\n",
    "average_scores = pd.DataFrame({\n",
    "    'Average Math Score': math_scores,\n",
    "    'Average Reading Score': reading_scores,\n",
    "    'Average Science Score': science_scores\n",
    "})\n",
    "\n",
    "average_scores.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41db47dd-2afd-46f2-8bb1-6a79de072374",
   "metadata": {},
   "source": [
    "#### Calculating Average Scores:\n",
    "   - **Data Grouping**: We begin by grouping the data by 'Country' using the `groupby` method.\n",
    "   - **Score Calculation**:\n",
    "     - For each subject (mathematics, reading, and science), we select the plausible values and calculate their mean for each country using the `mean()` method.\n",
    "     - We then compute the mean of these values across the columns, which represent different plausible values, to obtain a single average score per subject per country.\n",
    "     - These scores are rounded to the nearest whole number to facilitate easier interpretation.\n",
    "   - **Combining the Results**: All the calculated averages are combined into a single DataFrame named `average_scores`, which includes columns for average math, reading, and science scores, providing a comprehensive view of the performance across subjects for each country.\n",
    "   - **Displaying the Results**: The `reset_index()` method is used to transform the index (countries) back into a column, making the DataFrame suitable for further analysis or export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec99915f-b69d-47f0-af7a-ea48b66612bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iso_alpha(country_name):\n",
    "    try:\n",
    "        return pycountry.countries.lookup(country_name).alpha_3\n",
    "    except LookupError:\n",
    "        return None\n",
    "\n",
    "average_scores['iso_alpha'] = average_scores.index.map(get_iso_alpha)\n",
    "missing_iso = average_scores['iso_alpha'].isnull().sum()\n",
    "print(f\"Number of missing ISO codes: {missing_iso}\")\n",
    "missing_iso_countries = average_scores[average_scores['iso_alpha'].isnull()]\n",
    "missing_iso_countries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1684a24b-b14f-4153-b38e-d8c1457361a7",
   "metadata": {},
   "source": [
    "## Mapping Country Names to ISO Alpha-3 Codes\n",
    "\n",
    "The purpose of this segment is to integrate international standards into our dataset by converting country names into ISO alpha-3 codes, which are three-letter country codes defined by the International Organization for Standardization (ISO). This conversion facilitates consistent country identification across different datasets and enhances the compatibility of our data for global analysis.\n",
    "\n",
    "### Code Explanation:\n",
    "\n",
    "- **Function Definition (`get_iso_alpha`)**: We define a function called `get_iso_alpha` that attempts to find the ISO alpha-3 code for a given country name. This function uses the `pycountry` library, which contains data on country names and their corresponding codes.\n",
    "\n",
    "  - **Try-Except Block**: Within the function, we use a try-except block to handle situations where a country name might not be recognized or is missing from the `pycountry` database. If the country is found, the ISO alpha-3 code is returned; otherwise, `None` is returned.\n",
    "\n",
    "- **Mapping ISO Codes**: We apply the `get_iso_alpha` function to each country name in the index of the `average_scores` DataFrame. This is done using the `.map()` method, which transforms the values of the index according to the function defined.\n",
    "\n",
    "- **Counting Missing Codes**: After mapping the codes, we calculate the number of missing ISO codes by counting the `None` values in the new `iso_alpha` column. This is accomplished using the `.isnull().sum()` method, providing a quick count of how many country names could not be mapped to ISO codes.\n",
    "\n",
    "- **Identifying Countries with Missing Codes**: To further investigate and address the missing codes, we filter the `average_scores` DataFrame to list entries where the ISO code is `None`. This subset, stored in `missing_iso_countries`, helps identify specific countries that require further attention for correct ISO mapping.\n",
    "\n",
    "### Results:\n",
    "\n",
    "After mapping ISO codes to country names, we identified that 3 countries did not have corresponding ISO alpha-3 codes in the database: `QAZ`, `QUR`, `TAP`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5a96c9-30ab-468c-88ca-2aa97c247745",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_scores.loc[average_scores.index == 'QAZ', 'iso_alpha'] = 'AZE' \n",
    "average_scores.loc[average_scores.index == 'TAP', 'iso_alpha'] = 'TWN'\n",
    "\n",
    "average_scores = average_scores.drop(index='QUR')\n",
    "\n",
    "missing_iso_countries = average_scores[average_scores['iso_alpha'].isnull()]\n",
    "missing_iso_countries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af54a7c2-dafe-47b0-a053-6b8ed8c73615",
   "metadata": {},
   "source": [
    "## Correcting ISO Alpha-3 Codes for Specific Countries\n",
    "\n",
    "Following the initial mapping of country names to ISO alpha-3 codes, further scrutiny was required to ensure the accuracy of our dataset. A close examination of the PISA codebook provided insights into the correct identifiers for certain countries which were initially unrecognized.\n",
    "\n",
    "### Code Explanation:\n",
    "\n",
    "- **Manual Corrections**: Based on insights gained from the PISA codebook:\n",
    "  \n",
    "  - **Azerbaijan (QAZ)**: Initially not found under its common code, it was determined that 'QAZ' should be corrected to 'AZE', representing Azerbaijan. This correction was implemented using the `.loc[]` method on the `average_scores` DataFrame, specifying the index 'QAZ' and assigning 'AZE' to the 'iso_alpha' column.\n",
    "  \n",
    "  - **Taiwan (TAP)**: Similarly, 'TAP' was identified as the code used for Taiwan. It was corrected to 'TWN', aligning with the standard ISO alpha-3 code for Taiwan. This adjustment was also made using the `.loc[]` method.\n",
    "\n",
    "- **Removing Undefined Entries**: \n",
    "  - **QUR**: Despite exhaustive checks, no corresponding entry for 'QUR' was found in the PISA codebook. To maintain the integrity and accuracy of our analyses, rows associated with 'QUR' were removed from the dataset using the `.drop()` method.\n",
    "\n",
    "- **Final Verification**:\n",
    "  - After these corrections and the removal of the undefined entries, we again checked for any remaining entries with null ISO codes using the condition `average_scores[average_scores['iso_alpha'].isnull()]`. The result confirmed that all remaining entries now have valid ISO alpha-3 codes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "302f6754-bed9-4801-9dfd-879be45e8d3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'go' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trace1 \u001b[38;5;241m=\u001b[39m \u001b[43mgo\u001b[49m\u001b[38;5;241m.\u001b[39mChoropleth(\n\u001b[0;32m      2\u001b[0m     locations\u001b[38;5;241m=\u001b[39maverage_scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miso_alpha\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      3\u001b[0m     z\u001b[38;5;241m=\u001b[39maverage_scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Math Score\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m      4\u001b[0m     colorscale\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      5\u001b[0m         [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb(255,0,0)\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m      6\u001b[0m         [\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb(255,255,0)\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m      7\u001b[0m         [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb(0,128,0)\u001b[39m\u001b[38;5;124m'\u001b[39m] \n\u001b[0;32m      8\u001b[0m     ],\n\u001b[0;32m      9\u001b[0m     autocolorscale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     10\u001b[0m     reversescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     11\u001b[0m     marker_line_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     12\u001b[0m     marker_line_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[0;32m     13\u001b[0m     colorbar_title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMath Scores\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     14\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMath Score\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     15\u001b[0m     hoverinfo\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocation+z+name\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     18\u001b[0m trace2 \u001b[38;5;241m=\u001b[39m go\u001b[38;5;241m.\u001b[39mChoropleth(\n\u001b[0;32m     19\u001b[0m     locations\u001b[38;5;241m=\u001b[39maverage_scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miso_alpha\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     20\u001b[0m     z\u001b[38;5;241m=\u001b[39maverage_scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Reading Score\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m     hoverinfo\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocation+z+name\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     34\u001b[0m )\n\u001b[0;32m     36\u001b[0m trace3 \u001b[38;5;241m=\u001b[39m go\u001b[38;5;241m.\u001b[39mChoropleth(\n\u001b[0;32m     37\u001b[0m     locations\u001b[38;5;241m=\u001b[39maverage_scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miso_alpha\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     38\u001b[0m     z\u001b[38;5;241m=\u001b[39maverage_scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Science Score\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m     hoverinfo\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocation+z+name\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     52\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'go' is not defined"
     ]
    }
   ],
   "source": [
    "trace1 = go.Choropleth(\n",
    "    locations=average_scores['iso_alpha'],\n",
    "    z=average_scores['Average Math Score'], \n",
    "    colorscale=[\n",
    "        [0, 'rgb(255,0,0)'], \n",
    "        [0.5, 'rgb(255,255,0)'], \n",
    "        [1, 'rgb(0,128,0)'] \n",
    "    ],\n",
    "    autocolorscale=False,\n",
    "    reversescale=False,\n",
    "    marker_line_color='black',\n",
    "    marker_line_width=0.5,\n",
    "    colorbar_title='Math Scores',\n",
    "    name='Math Score',\n",
    "    hoverinfo='location+z+name'\n",
    ")\n",
    "\n",
    "trace2 = go.Choropleth(\n",
    "    locations=average_scores['iso_alpha'],\n",
    "    z=average_scores['Average Reading Score'],\n",
    "    colorscale=[\n",
    "        [0, 'rgb(255,0,0)'], \n",
    "        [0.5, 'rgb(255,255,0)'], \n",
    "        [1, 'rgb(0,128,0)'] \n",
    "    ],\n",
    "    autocolorscale=False,\n",
    "    reversescale=False,\n",
    "    marker_line_color='black',\n",
    "    marker_line_width=0.5,\n",
    "    colorbar_title='Reading Scores',\n",
    "    visible=False,\n",
    "    name='Reading Score',\n",
    "    hoverinfo='location+z+name'\n",
    ")\n",
    "\n",
    "trace3 = go.Choropleth(\n",
    "    locations=average_scores['iso_alpha'],\n",
    "    z=average_scores['Average Science Score'],\n",
    "    colorscale=[\n",
    "        [0, 'rgb(255,0,0)'], \n",
    "        [0.5, 'rgb(255,255,0)'],\n",
    "        [1, 'rgb(0,128,0)']\n",
    "    ],\n",
    "    autocolorscale=False,\n",
    "    reversescale=False,\n",
    "    marker_line_color='black',\n",
    "    marker_line_width=0.5,\n",
    "    colorbar_title='Science Scores',\n",
    "    visible=False,\n",
    "    name='Science Score',\n",
    "    hoverinfo='location+z+name'\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title_text='Global Average Scores by Subject',\n",
    "    title_x=0,\n",
    "    title_y=1,\n",
    "    title_xanchor='left',\n",
    "    title_yanchor='top',\n",
    "    geo=dict(\n",
    "        showframe=True,\n",
    "        showcoastlines=True,\n",
    "        projection_type='natural earth'\n",
    "    ),\n",
    "    updatemenus=[\n",
    "        dict(\n",
    "            buttons=list([\n",
    "                dict(\n",
    "                    args=[{'visible': [True, False, False]}],\n",
    "                    label='Mathematics',\n",
    "                    method='update'\n",
    "                ),\n",
    "                dict(\n",
    "                    args=[{'visible': [False, True, False]}],\n",
    "                    label='Reading',\n",
    "                    method='update'\n",
    "                ),\n",
    "                dict(\n",
    "                    args=[{'visible': [False, False, True]}],\n",
    "                    label='Science',\n",
    "                    method='update'\n",
    "                )\n",
    "            ]),\n",
    "            direction='down',\n",
    "            pad={'r': 10, 't': 10},\n",
    "            showactive=True,\n",
    "            x=0.5,\n",
    "            xanchor='center',\n",
    "            y=1.15,\n",
    "            yanchor='top'\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[trace1, trace2, trace3], layout=layout)\n",
    "\n",
    "fig.show(renderer='browser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4b7f49-6fc9-4cc5-89fb-6eb71fb8b5c8",
   "metadata": {},
   "source": [
    "## Interactive Choropleth Map Visualization\n",
    "\n",
    "In this section, we delve into creating an interactive choropleth map to visually represent the average scores in mathematics, reading, and science across various countries using Plotly's `Choropleth` map capabilities.\n",
    "\n",
    "### Implementation Details:\n",
    "\n",
    "- **Choropleth Map Setup**:\n",
    "  Each subject (math, reading, science) is represented by a separate `Choropleth` trace, which maps the ISO alpha-3 country codes to their respective average scores:\n",
    "  \n",
    "  - **Math Scores (`trace1`)**: Configured to display the average math scores. The color scale transitions from red for lower scores, through yellow for middle scores, to green for high scores, which provides a clear visual gradient of performance across countries.\n",
    "  \n",
    "  - **Reading Scores (`trace2`)**: Set up similarly but made invisible by default. This allows the user to switch to this view interactively without reloading the data.\n",
    "  \n",
    "  - **Science Scores (`trace3`)**: Also configured like the others but remains invisible initially, available for display via interactive controls.\n",
    "\n",
    "- **Color Scale and Appearance**:\n",
    "  All traces share a color scale that enhances the visual consistency across different subjects. The `autocolorscale` is set to `False` to use the custom color scale defined. The `reversescale` is also set to `False` to maintain the color direction from low to high scores.\n",
    "\n",
    "- **Interactivity**:\n",
    "  An interactive dropdown menu is implemented using `updatemenus`, allowing the viewer to switch between math, reading, and science scores. This interactivity is crucial for engaging users and enabling a comparative analysis without switching between different pages or views.\n",
    "\n",
    "- **Geographical and Styling Options**:\n",
    "  The layout of the map includes configurations like showing coastlines and a natural earth projection, which enhance geographical clarity. Additionally, titles and labels are carefully positioned to guide the viewer's understanding of the data being presented.\n",
    "\n",
    "- **Rendering the Map**:\n",
    "  The final step involves compiling the three traces into a single figure with the specified layout settings. This figure is then displayed in the browser using `fig.show(renderer='browser')` to ensure it renders interactively outside of the Jupyter environment.\n",
    "\n",
    "### Summary:\n",
    "\n",
    "This coding setup provides a robust platform for visualizing complex geographic data in an interactive manner. By leveraging Plotly's advanced mapping functionalities, the map offers not only a dynamic data exploration tool but also a clear visual representation of educational performance metrics across different countries, facilitating global educational insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4712b72e-bb81-4869-83af-fb73bfbb6352",
   "metadata": {},
   "source": [
    "## Global Analysis of PISA 2022 Average Scores by Subject\n",
    "\n",
    "### Overview\n",
    "The data visualized represents the average scores in Mathematics, Reading, and Science from the PISA 2022 survey across various countries. Each score reflects the proficiency of 15-year-old students in these subjects, offering insights into the educational outcomes and the effectiveness of educational systems globally.\n",
    "\n",
    "### Key Observations\n",
    "- **High Performers**: Singapore (SGP), Macao (MAC), and Hong Kong (HKG) standout with exceptionally high scores across all subjects, particularly in Mathematics and Science. This trend highlights the strong emphasis on STEM education in these regions, which is often supported by rigorous academic standards and substantial educational investments.\n",
    "- **Subject Variability**: Countries like Australia (AUS), Canada (CAN), and Germany (DEU) show strong performances across all subjects but do not lead in any specific area. This indicates a well-rounded educational approach that does not heavily favor any particular subject.\n",
    "- **Emerging Trends**: Countries like Vietnam (VNM) and Kazakhstan (KAZ) show promising results, especially in Science and Mathematics. This could be indicative of recent educational reforms or focused national strategies to enhance STEM education.\n",
    "- **Underperformance Issues**: Regions such as Dominican Republic (DOM), Guatemala (GTM), and Panama (PAN) lag significantly behind in all subjects. These lower scores might reflect broader socio-economic challenges, limited educational resources, or other systemic issues affecting educational outcomes.\n",
    "\n",
    "### Detailed Analysis\n",
    "- **Europe**: Most European countries like Estonia (EST), Finland (FIN), and Netherlands (NLD) score well above the global average, particularly in Science. This success can be attributed to strong educational policies, early childhood education quality, and substantial public investment in education.\n",
    "- **Asia**: Asian countries demonstrate high variability. While East Asia showcases top scores, South and Southeast Asian regions like Cambodia (KHM) and Philippines (PHL) underperform, highlighting a significant disparity in educational quality and access within the continent.\n",
    "- **Americas**: The United States (USA) shows strong Reading scores, aligning with its emphasis on literacy and critical thinking in curriculum. Conversely, countries in South America like Brazil (BRA) and Colombia (COL) show moderate to low scores, suggesting room for curriculum development and educational reforms.\n",
    "- **Challenges in Middle East and Africa**: Countries like Qatar (QAT) and Saudi Arabia (SAU) have lower scores compared to global standards despite high economic outputs, pointing towards potential mismatches between educational system outputs and the needs of a modern economy.\n",
    "\n",
    "### Implications for Policy and Practice\n",
    "- **Focus on STEM**: Countries lagging in Mathematics and Science might consider revising their STEM curricula and teacher training programs to boost performance and ensure their youth are equipped for modern challenges.\n",
    "- **Literacy and Reading**: Enhancing reading scores through comprehensive literacy programs from early grades could improve overall academic performance, as reading proficiency is foundational for success in other subjects.\n",
    "- **Addressing Inequities**: Lower-performing countries need targeted interventions to address educational inequities, such as improving school infrastructure, increasing teacher to student ratios, and providing additional support for disadvantaged students.\n",
    "\n",
    "### Conclusion\n",
    "This analysis underscores the importance of continuous monitoring and evaluation of educational systems worldwide. By understanding the strengths and weaknesses revealed through such data, stakeholders can implement focused strategies that enhance educational outcomes and foster an environment where all students can excel in their academic endeavors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b07f8f-a64f-46f9-a22b-a5dacae5874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Country'] != 'QUR']\n",
    "\n",
    "df['Average Math Score'] = df[['Math_Score_PV1', 'Math_Score_PV2', 'Math_Score_PV3', 'Math_Score_PV4', 'Math_Score_PV5']].mean(axis=1)\n",
    "df['Average Reading Score'] = df[['Reading_Score_PV1', 'Reading_Score_PV2', 'Reading_Score_PV3', 'Reading_Score_PV4', 'Reading_Score_PV5']].mean(axis=1)\n",
    "df['Average Science Score'] = df[['Science_Score_PV1', 'Science_Score_PV2', 'Science_Score_PV3', 'Science_Score_PV4', 'Science_Score_PV5']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4b5c4a-b6c5-4539-bfab-02f6fac475d1",
   "metadata": {},
   "source": [
    "## Integration of Key Calculations into Main DataFrame\n",
    "\n",
    "### Exclusion of Undefined Country Entries\n",
    "In our main DataFrame, we continue with data cleansing by excluding entries associated with the undefined country identifier 'QUR'. This measure is crucial for maintaining the dataset's integrity, ensuring that subsequent analyses are based on verifiable and relevant data.\n",
    "\n",
    "### Implementation of Average Score Calculations\n",
    "Previously, we had computed average scores for Math, Reading, and Science in a separate DataFrame to facilitate analysis. Recognizing the importance and utility of these calculations, we've now integrated this process directly into our main DataFrame. This integration enhances data accessibility and streamlines operations by allowing direct access to average scores during our analytical procedures.\n",
    "\n",
    "By performing these operations in the main DataFrame, we ensure that all subsequent analyses utilize the most refined and accurate data representation. This approach minimizes redundancy and maximizes efficiency, enabling more straightforward and effective data manipulation and analysis going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e47066f-871e-400f-954a-098f35d99ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "variables = [\n",
    "    'Test_Administration_Mode', 'Grade', 'Gender', 'Student_father’s_country_of_birth',\n",
    "    'Mother_Education_Level', 'Father_Education_Level', 'Immigration_Status', \n",
    "    'Weekly_Study_Time', 'Math_Motivation'\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(variables), ncols=1, figsize=(10, 45))\n",
    "\n",
    "total_observations = df.shape[0]\n",
    "\n",
    "for i, var in enumerate(variables):\n",
    "    data = df[var].value_counts().sort_index()\n",
    "    norm = plt.Normalize(data.min(), data.max())\n",
    "    colors = plt.cm.viridis(norm(data.values))\n",
    "    colors = colors.tolist() \n",
    "\n",
    "    bars = sns.barplot(x=data.index, y=data.values, palette=colors, ax=axes[i])\n",
    "    axes[i].set_title(f'Distribution of {var}', fontsize=16)\n",
    "    axes[i].set_ylabel('Counts', fontsize=14)\n",
    "    axes[i].set_xlabel(var, fontsize=14)\n",
    "    axes[i].tick_params(axis='x', rotation=45) \n",
    "\n",
    "    for bar in bars.patches:\n",
    "        height = bar.get_height()\n",
    "        percentage = (height / total_observations) * 100\n",
    "        axes[i].text(bar.get_x() + bar.get_width() / 2., height,\n",
    "                     f'{int(height)}\\n({percentage:.2f}%)',\n",
    "                     ha='center', va='bottom', color='black', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1134a4a-ea7e-4dd3-814c-14dd0d12e450",
   "metadata": {},
   "source": [
    "## Data Visualization: Distribution of Key Variables in PISA 2022 Data\n",
    "\n",
    "In this section of our analysis, we focus on visualizing the distribution of several key sociocultural and educational variables within the PISA 2022 dataset. By creating a series of bar plots, we aim to illustrate the frequency and proportions of various categories within each variable, providing insights into the demographic and educational landscapes of the students assessed.\n",
    "\n",
    "### Code Explanation:\n",
    "\n",
    "1. **Setting Aesthetic Style**:\n",
    "   - We begin by setting the aesthetic style of our plots to \"whitegrid\" using `sns.set()`. This style provides a clean and clear background with grid lines that help in gauging the scale of the bar plots.\n",
    "\n",
    "2. **Variables to Plot**:\n",
    "   - We define a list named `variables` that contains the names of the sociocultural and educational variables we intend to explore. These variables include 'Test Administration Mode', 'Grade', 'Gender', 'Student father’s country of birth', 'Mother Education Level', 'Father Education Level', 'Immigration Status', 'Weekly Study Time', and 'Math Motivation'.\n",
    "\n",
    "3. **Creating Figure for Subplots**:\n",
    "   - A figure `fig` and a series of axes `axes` are created using `plt.subplots()`. This arrangement allows us to plot multiple subplots in a single column layout, with each subplot dedicated to a different variable. The figure size is set to 10 inches in width and 45 inches in height to accommodate all subplots vertically.\n",
    "\n",
    "4. **Calculating Total Observations**:\n",
    "   - `total_observations` is calculated using `df.shape[0]`, which provides the total number of rows (students) in our dataset. This value is crucial for calculating percentages in subsequent annotations.\n",
    "\n",
    "5. **Iterating Over Variables**:\n",
    "   - We loop through each variable using a for loop. Within each iteration, the following steps are executed for the current variable:\n",
    "     - **Data Extraction**: The frequency of each category within the variable is obtained using `df[var].value_counts().sort_index()`. This function sorts the categories to maintain a consistent order across the plots.\n",
    "     - **Normalization and Color Palette**: A normalization object `norm` is created using `plt.Normalize()`, which scales the counts to a range that is used to map colors. We then create a color palette `colors` that corresponds to the normalized data values, using a 'viridis' colormap which offers good color differentiation.\n",
    "     - **Bar Plot Creation**: A bar plot is generated using `sns.barplot()`, with the categories on the x-axis and their counts on the y-axis. The `palette` argument is supplied with the list of colors created in the previous step.\n",
    "     - **Plot Customization**: Titles and labels are set for each subplot to clearly identify the variable being displayed. The x-axis labels are rotated for better readability.\n",
    "     - **Annotation**: Each bar is annotated with its count and the corresponding percentage of the total observations, providing a clear quantitative measure of each category's prevalence.\n",
    "\n",
    "6. **Layout Adjustment**:\n",
    "   - `plt.tight_layout()` is called to automatically adjust the subplots' parameters to give some padding and prevent overlap between them.\n",
    "\n",
    "7. **Displaying the Plots**:\n",
    "   - Finally, `plt.show()` is used to display the figure with all the subplots. This command ensures that all our visual configurations are rendered properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307244a4-9ffe-4b07-811d-7f82c097ad82",
   "metadata": {},
   "source": [
    "## Detailed Analysis of Variable Distributions with Extended Context\n",
    "\n",
    "Our analysis delves deeper into the key sociodemographic and educational variables from the PISA 2022 dataset. Each variable's distribution provides a nuanced understanding of the educational landscape.\n",
    "\n",
    "### Test Administration Mode\n",
    "- **Distribution Breakdown**: \n",
    "  - `Paper` (1): 17,307 instances.\n",
    "  - `Computer` (2): 454,102 instances.\n",
    "- **Analysis**: The overwhelming preference for computer-based testing (454,102 vs. 17,307 for paper) reflects modern educational trends and digital integration in testing environments. This shift towards digital platforms can influence test performance, potentially benefiting students more familiar with digital interfaces.\n",
    "\n",
    "### Grade\n",
    "- **Distribution Breakdown** (selected):\n",
    "  - Grade 10: 278,958\n",
    "  - Grade 9: 133,640\n",
    "  - Other grades show varying counts, with significant placeholders for non-standard categories like 'Invalid' and 'Missing'.\n",
    "- **Analysis**: The prevalence of students in Grades 9 and 10 suggests these are critical years for assessment across countries, possibly coinciding with the culmination of lower secondary education or transition to higher levels. The presence of grades like 'Invalid' and 'Missing' could indicate data integrity issues or variations in educational systems, such as differing numbers of grades across countries, which could affect comparative analysis.\n",
    "\n",
    "### Gender\n",
    "- **Distribution Breakdown**:\n",
    "  - `Female` (1): 243,605\n",
    "  - `Male` (2): 227,804\n",
    "- **Analysis**: The near balance in gender distribution ensures that the dataset adequately represents both genders. This parity is crucial for analyzing gender-based differences in educational outcomes without bias.\n",
    "\n",
    "### Student Father’s Country of Birth\n",
    "- **Distribution Breakdown**:\n",
    "  - Country of test (1): 392,822\n",
    "  - Other country (2): 75,865\n",
    "  - Unknown or stateless (3): 2,722\n",
    "- **Analysis**: The dominant number of fathers born in the country of test suggests minimal migration influence for the majority. However, the significant number from 'Other countries' and a small proportion from 'Unknown or stateless' backgrounds can provide insights into the integration and educational challenges faced by students from diverse familial origins.\n",
    "\n",
    "### Parent Education Level (Mother and Father)\n",
    "- **Assumed Levels**:\n",
    "  - 1: No formal education\n",
    "  - 2: Primary education\n",
    "  - 3: Some secondary\n",
    "  - 4: Secondary completed\n",
    "  - 5: Some college\n",
    "  - 6: Bachelor's degree\n",
    "  - 7: Master's degree\n",
    "  - 8: Doctorate or higher\n",
    "- **Analysis**: The varied educational levels of parents, with a higher frequency at the secondary and some college levels, underscore the influence of parental education on student's educational access and attitudes. Higher parental education often correlates with better educational support at home, potentially enhancing student performance.\n",
    "\n",
    "### Immigration Status\n",
    "- **Distribution Breakdown**:\n",
    "  - Native (1): 414,227\n",
    "  - Second-Generation (2): 29,932\n",
    "  - First-Generation (3): 27,250\n",
    "- **Analysis**: The predominance of native students in the dataset highlights a primarily stable resident student population, with fewer first and second-generation immigrants. This distribution is essential for analyzing the impact of immigration on educational outcomes, where native students might exhibit different performance patterns compared to their immigrant peers.\n",
    "\n",
    "### Weekly Study Time\n",
    "- **Definition**: Amount of time spent on homework per week in hours.\n",
    "- **Analysis**: The mode of 10 hours per week indicates a significant commitment from students towards homework, suggesting a correlation between time investment and educational outcomes. Variations in study time can reflect different educational pressures, personal discipline, and possibly the effectiveness of school homework policies.\n",
    "\n",
    "### Math Motivation\n",
    "- **Distribution Breakdown**:\n",
    "  - No motivation (0): 448,892\n",
    "  - Has motivation (1): 22,517\n",
    "- **Analysis**: The stark contrast in math motivation with a vast majority displaying no motivation poses significant challenges. It is crucial to address these motivational gaps through targeted interventions, as motivation is a key driver of engagement and achievement in mathematics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5945758f-26a8-45cb-8ab9-bca3f53024af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "variables = [\n",
    "    'Sense_of_Belonging', 'Bullying_Frequency', \n",
    "    'Feeling_of_Safety', 'Disciplinary_Climate', \n",
    "    'Economic_Social_Cultural_Status', 'Access_to_ICT_at_Home'\n",
    "]\n",
    "\n",
    "titles = [\n",
    "    'Distribution of Sense of Belonging Scores',\n",
    "    'Distribution of Bullying Frequency Scores',\n",
    "    'Distribution of Feeling of Safety Scores',\n",
    "    'Distribution of Disciplinary Climate Scores',\n",
    "    'Distribution of Economic Social Cultural Status Scores',\n",
    "    'Distribution of Access to ICT at Home Scores'\n",
    "]\n",
    "\n",
    "for variable, title in zip(variables, titles):\n",
    "    fig = px.histogram(df, x=variable, title=title, nbins=30, labels={variable: 'Score'},\n",
    "                       marginal='box', color_discrete_sequence=['#636EFA']) \n",
    "    fig.update_layout(xaxis_title=variable, yaxis_title='Frequency', title_x=0.5)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6f02ae-46ce-4216-8951-0f2a432722cb",
   "metadata": {},
   "source": [
    "### Code Explanation:\n",
    "\n",
    "1. **Setting Aesthetic Style**:\n",
    "   - We initialize the aesthetic style to \"whitegrid\" using `sns.set()`. This setting ensures that all our plots have a uniform, clean, and visually appealing background that enhances readability and visual consistency.\n",
    "\n",
    "2. **Variables and Titles Setup**:\n",
    "   - `variables`: A list containing the key variables of interest. These include psychosocial factors like 'Sense of Belonging', 'Bullying Frequency', 'Feeling of Safety', and educational environment indicators such as 'Disciplinary Climate', 'Economic Social Cultural Status', and 'Access to ICT at Home'.\n",
    "   - `titles`: Corresponding titles for each histogram to be plotted. Each title is carefully crafted to clearly represent the variable being analyzed, enhancing the interpretability of the visualizations.\n",
    "\n",
    "3. **Loop Through Variables for Plotting**:\n",
    "   - We employ a loop to iterate over each variable and its corresponding title. This approach facilitates the efficient generation of multiple histograms in a systematic manner.\n",
    "   - Within the loop, the following steps are executed:\n",
    "     - **Histogram Generation**: Utilizing Plotly's `px.histogram`, we generate a histogram for each variable. The function is configured to:\n",
    "       - Display the variable on the x-axis as 'Score'.\n",
    "       - Set the number of bins to 30, optimizing the resolution of the distribution.\n",
    "       - Include a box plot (`marginal='box'`) to provide a summary of the distribution's central tendency and variability.\n",
    "       - Use a consistent color (`color_discrete_sequence=['#636EFA']`) to maintain visual coherence across all plots.\n",
    "     - **Layout Customization**: Each plot's layout is updated to:\n",
    "       - Set the titles of the x-axis and y-axis to enhance clarity.\n",
    "       - Center the main title above the plot for balanced aesthetics.\n",
    "     - **Interactive Display**: The `fig.show()` function is called to render each plot interactively, allowing users to engage directly with the data, such as zooming in on details or hovering to see specific values.\n",
    "\n",
    "4. **Quantitative vs. Categorical Variables**:\n",
    "   - In contrast to our earlier use of bar plots for categorical variables, these histograms are specifically designed for quantitative data. This distinction is crucial as histograms allow us to visualize the distribution of numerical data, providing insights into trends, spread, and outliers, which are not typically evident in categorical data visualization. Histograms are particularly useful for examining the distribution characteristics and central tendencies within continuous data sets, whereas bar plots are more suited to showing frequency counts in discrete categories.\n",
    "\n",
    "5. **Execution and Display**:\n",
    "   - The execution of the loop ensures that each variable is visualized in sequence, and each histogram is displayed immediately after creation, providing immediate feedback and visual data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3137d2-69f0-497b-a510-b33a5f23be22",
   "metadata": {},
   "source": [
    "## Detailed Analytical Insights into Psychosocial and Educational Variables\n",
    "\n",
    "This comprehensive analysis extends our understanding of the distributions of key educational and psychosocial variables from the PISA 2022 dataset, focusing on deeper interpretations and potential educational implications.\n",
    "\n",
    "### Sense of Belonging\n",
    "- **Statistical Summary**:\n",
    "  - Minimum: -3.5037, Maximum: 3.1727\n",
    "  - Quartiles: Q1 = -0.6313, Median = -0.2934, Q3 = 0.3661\n",
    "  - Outliers: Lower fence = -2.1267, Upper fence = 1.844\n",
    "- **Distribution Insights**:\n",
    "  - The distribution's central concentration between -2 and 2 suggests that most students feel a moderate sense of belonging, which is crucial for emotional and academic success. The sharp peak around 2.25 to 2.75 indicates a subset of students with exceptionally high belonging, possibly due to strong peer or school support systems. Conversely, the low distribution tails beyond -2 highlight a vulnerable group with a severe lack of belonging, potentially leading to feelings of isolation and disengagement from school activities. Schools might utilize this data to develop targeted interventions to enhance community building and inclusive practices, ensuring every student feels part of the school community.\n",
    "\n",
    "### Bullying Frequency\n",
    "- **Statistical Summary**:\n",
    "  - Minimum: -1.228, Maximum: 4.6939\n",
    "  - Quartiles: Q1 = -1.228, Median = -0.391, Q3 = 0.5222\n",
    "  - Outliers: Lower fence = -1.228, Upper fence = 3.1472\n",
    "- **Distribution Insights**:\n",
    "  - The pronounced peak at the lowest scoring range (-1.4 to -1.2) with 212,788 students suggests a majority reporting minimal bullying, reflecting effective anti-bullying policies or underreporting issues. The distribution from -0.6 to 2 with a peak around 0.2 to 0.4, however, indicates a significant number still experience bullying at varying frequencies. The extreme values nearing 4.7 represent severe cases, which might include chronic bullying scenarios. These insights could be instrumental for schools and policymakers to further strengthen their anti-bullying strategies, focusing on the areas and student groups where bullying is more prevalent.\n",
    "\n",
    "### Feeling of Safety\n",
    "- **Statistical Summary**:\n",
    "  - Minimum: -2.7886, Maximum: 1.1687\n",
    "  - Quartiles: Q1 = -0.756, Median = -0.039, Q3 = 1.1246\n",
    "  - Outliers: Lower fence = -2.7886, Upper fence = 1.1687\n",
    "- **Distribution Insights**:\n",
    "  - The bipolar distribution with significant accumulation at both extremes suggests a polarized student perception regarding safety. This polarization can be indicative of differing school environments, where some schools are perceived as safe havens while others may be viewed as lacking in safety measures. Such disparities could be critical for district-level educational authorities to investigate, aiming to standardize safety measures and improve student perceptions of safety across all schools.\n",
    "\n",
    "### Disciplinary Climate\n",
    "- **Statistical Summary**:\n",
    "  - Minimum: -2.6027, Maximum: 2.2258\n",
    "  - Quartiles: Q1 = -0.5304, Median = -0.0611, Q3 = 0.6888\n",
    "  - Outliers: Lower fence = -2.356, Upper fence = 2.2258\n",
    "- **Distribution Insights**:\n",
    "  - The quasi-normal distribution suggests that while most students perceive the disciplinary climate as fair, the range of perceptions is quite broad. This variance might reflect differences in implementation of disciplinary policies across schools or regions. The outliers, particularly those on the lower end, might indicate excessively harsh disciplinary environments, which could impact student morale and engagement negatively. Tailoring disciplinary measures to be both firm and fair could help improve perceptions and enhance overall student compliance and satisfaction.\n",
    "\n",
    "### Economic Social Cultural Status\n",
    "- **Statistical Summary**:\n",
    "  - Minimum: -6.3958, Maximum: 7.38\n",
    "  - Quartiles: Q1 = -0.9944, Median = -0.1177, Q3 = 0.6313\n",
    "  - Outliers: Lower fence = -3.4328, Upper fence = 3.0654\n",
    "- **Distribution Insights**:\n",
    "  - The socioeconomic status exhibits a gradual increase in frequency towards the median, with a notable peak in higher socioeconomic brackets. This trend suggests that while a significant portion of students come from middle to upper socioeconomic backgrounds, there exists a small but significant population at the extreme low end, potentially lacking basic educational resources. These insights are crucial for policymakers to allocate resources effectively, ensuring that lower socioeconomic groups receive the necessary support to bridge educational disparities.\n",
    "\n",
    "### Access to ICT at Home\n",
    "- **Statistical Summary**:\n",
    "  - Most observations are concentrated around 0-0.5, indicating widespread access to ICT.\n",
    "- **Distribution Insights**:\n",
    "  - The predominant concentration near zero suggests that the majority of students have good access to ICT, essential for engaging with modern educational content and methods. However, the minor peaks at extreme negative values highlight the presence of a digital divide, where a small group of students lacks adequate access. This situation calls for targeted technological interventions to ensure all students can benefit from digital learning opportunities.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
